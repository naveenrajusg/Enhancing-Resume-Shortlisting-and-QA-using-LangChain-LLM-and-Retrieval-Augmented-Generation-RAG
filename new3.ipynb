{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4276a32d-1ca0-4270-95ae-3a7a6f964bb5",
   "metadata": {},
   "source": [
    "## Load docs, create chroma db using embedings, use it as a retreiever, create RetrievalQA, did not split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b09c67-bd1b-4c2b-a641-7d88ff76a5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navee\\anaconda3\\envs\\llm_project\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7124662c-8002-450b-b766-8a7c318a003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'E:/LangChain course/Lang Chain for LLM application development/cv_read/'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory,show_progress=True) #unstructuredloader by default has used this auto identify file type and load it, mode=\"single\", strategy='fast'(other option is strategy='hi_res' that use yolo varient if mode is elements)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory) \n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2685ac7b-b7c8-476e-b868-342500f479dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | LinkedIn - https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\\n\\nSUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Deep Learning, Computer Vision, Data Science.\\n\\nTECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework: Keras, TensorFlow Other libraries: Numpy, Pandas, Matplotlib, scikit-learn Other relevant skills : MLOPS,Machine Learning, Deep Learning/Computer Vision, Convolutional Neural Network(CNN), Recurrent Neural Network(RNN), Long Short Term Memory(LSTM), Vision Transformer’s, Object - Detection, Segmentation, Classification, image processing, Generative AI - LLM.\\n\\nProgramming languages: Python, R, C++, SQL Image processing libraries: Open CV, scikit-image Version Control : Git and GitHub\\n\\nEDUCATION Illinois Institute of Technology May 2024 Master of Science Artificial Intelligence - GPA : 3.833 / 4 Courses : Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\\n\\nVisvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\\n\\nJuly 2018\\n\\nWORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\\n\\nSeptember 2023 - Present\\n\\nEngineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \\uf06c Worked on a deep learning based number plate detection and character recognition algorithm with overall accuracy of 90%. \\uf06c\\n\\nImproved the accuracy of crowd detection and human trespass detection algorithms by 25%.\\n\\nMay 2021 – June 2022\\n\\nAI Engineer ( Telerad Tech Pvt Ltd., India) \\uf06c\\n\\nAugust 2018 – May 2021\\n\\nLed a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Engineered a tailored U-Net architecture to achieve a remarkable 90% sensitivity and specificity in lymph node segmentation from mammograms. Devised a cutting-edge deep learning model and image processing logic, delivering an impressive 95% F1 score for detecting mammography asymmetry. Created a robust image processing algorithm and CNN classifier to effectively reduce false positives in mammograms' calcification detection, achieving an impressive 92% recall and 90% precision. Designed custom CNN architectures for precise segmentation and detection of various lung conditions, incorporating advanced image processing techniques for accurate localization and quantification. Achieved outstanding results with a 95% dice score, 90% MAP score, and an overall accuracy of 93%.\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nINTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India) \\uf06c\\n\\nTailored a deep learning architecture for precise mammogram lesion segmentation, augmented by advanced image processing techniques to enhance both specificity and sensitivity. Achieved exceptional results with an impressive 94% IOU score and an outstanding F1 score of 92.5%.\\n\\nJuly 2018\\n\\nACADEMIC PROJECT \\uf06c\\n\\nImage classification using hierarchical based shifted window Vision Transformers : Trained with various hyperparameter fine-tuning strategies for both the Hierarchical-Based Shifted Window Vision Transformer and standard Vision Transformers on the Food-101 dataset. Achieved competitive performance compared to the standard Vision Transformers, with reduced computational complexity. Seismic image salt region segmentation : Attained good seismic image salt region segmentation using a custom deep learning UNet backbone, surpassing ResNet 50, ResNet 101, and VGG16 with significantly fewer parameters (7.86x, 12.45x, and 5.73x reduction, respectively). Achieved IOU of 84.168 and F1 score of 81%. Unpaired image-to-image translation using Cycle GAN : Pioneered Cycle GAN, eliminating the need for paired data, thus enabling real-world applications with unaligned and unpaired training data. Achieved impressive results, including absolute mean square errors of 68.22 (Domain A to Domain B) and 91.73 (B to A), as well as SSIM scores of 87.86 (A to B) and 49.08 (B to A).\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nCERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On! | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\\n\\nPUBLICATIONS \\uf06c\\n\\nPneumothorax Detection and Classification on Chest Radiographs using Artificial Intelligence - Lattice the Machine Learning Journal Volume-2 Issue-1, ISSN 2582-8312 Pneumonia Detection and Classification on Chest Radiographs using Deep Learning - Lattice the Machine Learning Journal Volume-2 Issue-2, ISSN 2582-8312\\n\\nJanuary - March 2021\\n\\n\\uf06c\\n\\nApril - June 2021\", metadata={'source': 'E:\\\\LangChain course\\\\Lang Chain for LLM application development\\\\cv_read\\\\CV_DL_NAVEEN RAJU SREERAMA RAJU GOVINDA RAJU.pdf'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e75146-4595-47d1-bb2b-f4ddd66d7a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# def split_docs(documents, chunk_size=1000, chunk_overlap=100):\n",
    "#   text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#   docs = text_splitter.split_documents(documents)\n",
    "#   return docs\n",
    "\n",
    "# # docs = split_docs(documents)\n",
    "# # print(len(docs))\n",
    "\n",
    "docs = documents\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bade26fa-ed6c-40b5-94c9-15f08e9caf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0daa4b02-ba63-4c73-ae45-eca137e67cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9cbecf-b8af-4242-90d1-ab0944f2f49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45039b6a-7e69-49e3-b207-fba04e2da40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004862199989200547,\n",
       " 0.004893690799325886,\n",
       " -0.016362689388648106,\n",
       " -0.024474752996844814,\n",
       " -0.017320012624161614,\n",
       " 0.012539689393212747,\n",
       " -0.019108697717332413,\n",
       " 0.009107178051034791,\n",
       " -0.010215658665266041,\n",
       " -0.026981431913634602,\n",
       " 0.02282462821328356,\n",
       " 0.010316429630196154,\n",
       " -0.023454446278435487,\n",
       " -0.0066382889445857265,\n",
       " 0.008036485850160407,\n",
       " 0.002742860225560473,\n",
       " 0.025079378786425494,\n",
       " -0.012092518585581333,\n",
       " 0.012911282442807861,\n",
       " 0.01303724661463179,\n",
       " -0.01051797156005638,\n",
       " -0.0034828972903042975,\n",
       " 0.003977304778785006,\n",
       " 0.008603323226384225,\n",
       " -0.020670648605071743,\n",
       " -0.0018516668298367996,\n",
       " 0.012231078895190883,\n",
       " -0.019159084131120042,\n",
       " 0.0305588020999762,\n",
       " -0.03101227051083914,\n",
       " 0.003586817289680816,\n",
       " -0.007822347782514559,\n",
       " -0.00600217187921842,\n",
       " -0.017748290622098453,\n",
       " 0.004843305316860829,\n",
       " -0.01566988784060036,\n",
       " 0.001340726503058838,\n",
       " -0.015581713944778442,\n",
       " 0.01953697571526925,\n",
       " -0.016123358114108442,\n",
       " 0.00733738763020371,\n",
       " 0.008389184227415804,\n",
       " 0.01144380491676712,\n",
       " -0.013906395954323376,\n",
       " -0.03322923173930164,\n",
       " 0.011317841676265764,\n",
       " 0.0043016616131921126,\n",
       " -0.012709740512947635,\n",
       " -0.001415517409937161,\n",
       " 0.028392225422656188,\n",
       " 0.025381691681215835,\n",
       " 0.0006928005585175124,\n",
       " -0.016274513630181044,\n",
       " 0.017471170002879354,\n",
       " -0.016375284595111155,\n",
       " 0.008496253726900015,\n",
       " -0.033103268498800284,\n",
       " 0.03108784920019801,\n",
       " 0.032423064019860726,\n",
       " -0.02140123763515378,\n",
       " 0.0032782060931670224,\n",
       " 0.0055424044681400995,\n",
       " -0.007778260368942314,\n",
       " 0.006068302766746147,\n",
       " 0.006203713809078647,\n",
       " 0.016916928764441157,\n",
       " 0.0036214573670832035,\n",
       " 0.007708980679798825,\n",
       " -0.018831578960758457,\n",
       " 0.02609968550483482,\n",
       " 0.024638505582025606,\n",
       " 0.005488869718397994,\n",
       " -0.0016013139347347185,\n",
       " -0.0047708763604402925,\n",
       " 0.018415898031929813,\n",
       " 0.003116027908794112,\n",
       " -0.011418612641195878,\n",
       " -0.0019603106137135694,\n",
       " 0.005092083927570351,\n",
       " 0.0022138126596773773,\n",
       " 0.036201976136062554,\n",
       " -0.0274097080489263,\n",
       " -0.008546639209365072,\n",
       " 0.015430557497383271,\n",
       " 0.018907155787472187,\n",
       " 0.0018815832391542574,\n",
       " -0.003328591575632079,\n",
       " 0.020884787604040164,\n",
       " -0.014448040123653378,\n",
       " -0.02505418651085425,\n",
       " 0.012602671013463426,\n",
       " 0.013830818196287077,\n",
       " 0.006820936202106235,\n",
       " 0.008559235347150692,\n",
       " -0.015229015567523044,\n",
       " -0.0029837659009080164,\n",
       " -0.008653708708849281,\n",
       " 0.016866544213298675,\n",
       " 0.004144206631242845,\n",
       " -0.04799218089553098,\n",
       " -0.012665653565036674,\n",
       " -0.001977630652414763,\n",
       " -0.017345206762378,\n",
       " -0.013314366302528316,\n",
       " -0.009050494034015638,\n",
       " 0.0006719377687526217,\n",
       " -0.005646324234685975,\n",
       " -0.0005416440529146814,\n",
       " 0.025797372610044483,\n",
       " 0.0048968400666029345,\n",
       " -0.012848300822557184,\n",
       " 0.018818981891650266,\n",
       " -0.015493539117633948,\n",
       " -0.03232229491757576,\n",
       " 0.00467640346440299,\n",
       " -0.008590726157276032,\n",
       " 0.005173960220160746,\n",
       " -0.0071295471657893865,\n",
       " -0.016891736488869918,\n",
       " -0.023378869451721757,\n",
       " 0.013591486921747414,\n",
       " 0.03680660192564324,\n",
       " 0.015392768152703835,\n",
       " -0.026553155778342906,\n",
       " 0.0076774894040122,\n",
       " 0.0066193942722460085,\n",
       " -0.0382677837110976,\n",
       " -0.015014876568554623,\n",
       " 0.0003097920553273656,\n",
       " -0.01662721293875901,\n",
       " 0.04197111713794056,\n",
       " 0.002437398296323727,\n",
       " 0.015027473637662816,\n",
       " -0.011897274258952631,\n",
       " -0.03201998016014027,\n",
       " 0.019725920576021287,\n",
       " -0.008055380522500126,\n",
       " 0.028266262182154833,\n",
       " -0.011368227158730822,\n",
       " -0.016828754868619237,\n",
       " 0.004635465318107792,\n",
       " 0.024462157790381765,\n",
       " 0.018100988068031277,\n",
       " -0.016350092319539912,\n",
       " -0.008193940832109675,\n",
       " 0.005813226087143814,\n",
       " 0.0051550655478210285,\n",
       " -0.021237485049972988,\n",
       " 0.011047019591600763,\n",
       " -0.0210737324647922,\n",
       " 0.011393419434302063,\n",
       " 0.020544685364570388,\n",
       " -7.956381136828674e-05,\n",
       " 0.000469608551176963,\n",
       " 0.013503312094602921,\n",
       " 0.007280703613184556,\n",
       " -0.0010651808667242286,\n",
       " 0.027157781567923586,\n",
       " 0.010196763992926322,\n",
       " -0.003454555281794721,\n",
       " 0.004777174429333103,\n",
       " 0.002245303469802716,\n",
       " -0.0023759908440502873,\n",
       " -0.03514388193561894,\n",
       " 0.01054946237018172,\n",
       " 0.02407166913712436,\n",
       " 0.027082202878564714,\n",
       " -0.006414703075108733,\n",
       " -0.006272993963883422,\n",
       " -0.029097622177166983,\n",
       " -0.012552285530998368,\n",
       " 0.010058203683316773,\n",
       " -0.03466521938653961,\n",
       " 0.019247258026941962,\n",
       " -0.01944879995680219,\n",
       " 0.005756542535785948,\n",
       " -0.0035899663241272216,\n",
       " 0.023353675313505372,\n",
       " -0.01476294915622934,\n",
       " -0.013742643369142583,\n",
       " -0.03164209043863621,\n",
       " 0.008817461294030073,\n",
       " 0.006865023615678482,\n",
       " 0.025973722264333466,\n",
       " 0.008830057431815695,\n",
       " 0.001859539648783456,\n",
       " 0.012508198583087408,\n",
       " -0.0017430231623753515,\n",
       " 0.0034073190666067126,\n",
       " -0.020103812160170497,\n",
       " 0.01871821092672015,\n",
       " 0.02854338280137393,\n",
       " 0.031037464649055525,\n",
       " 0.011091106539511723,\n",
       " -0.6904827932621684,\n",
       " -0.0054636769771654655,\n",
       " 0.020242372469780046,\n",
       " 0.02436138682545165,\n",
       " 0.010826582989400818,\n",
       " 0.00906309017180126,\n",
       " 0.017634922588060147,\n",
       " 0.02982821120724902,\n",
       " 0.006852427012231575,\n",
       " 0.029903789896607892,\n",
       " -0.012306656653227182,\n",
       " 0.017886850931708,\n",
       " -0.011324139279497289,\n",
       " -0.007066565545538708,\n",
       " 0.0012533391091489293,\n",
       " -0.008250624849128828,\n",
       " 0.0018532414634753237,\n",
       " -0.021048540189220956,\n",
       " -0.018932349925688572,\n",
       " 0.0198266915409514,\n",
       " 0.0011911445728868704,\n",
       " 0.0307099576160488,\n",
       " -0.021237485049972988,\n",
       " -0.011771311018451275,\n",
       " 0.004163101303582563,\n",
       " 0.008848952104155412,\n",
       " 0.006802041529766518,\n",
       " -0.009132370326606035,\n",
       " -0.016060376493857765,\n",
       " 0.024877836856565267,\n",
       " -0.00792941728199877,\n",
       " 0.019637746680199367,\n",
       " -0.009189054343625188,\n",
       " 0.00538180068457507,\n",
       " 0.06781887243648238,\n",
       " 0.018415898031929813,\n",
       " -0.004695298136742707,\n",
       " 0.018881963511900943,\n",
       " 0.003284504394890476,\n",
       " 0.030206102791398234,\n",
       " -0.011210772176781554,\n",
       " -0.014070148539504166,\n",
       " 0.004909436670049842,\n",
       " -0.015317189463344965,\n",
       " 0.001025817121236912,\n",
       " 0.008754479673779396,\n",
       " 0.012073623913241615,\n",
       " -0.004002497520017534,\n",
       " 0.0003483684548915899,\n",
       " 0.008552937743919168,\n",
       " 0.022509718249385028,\n",
       " -0.016085568769429008,\n",
       " 0.017886850931708,\n",
       " 0.01566988784060036,\n",
       " 0.0021823216167213954,\n",
       " 0.009510261910755246,\n",
       " 0.01685394714419048,\n",
       " 0.010845477661740537,\n",
       " -0.006430448480171403,\n",
       " 0.0037474210732458455,\n",
       " -0.002317732659053896,\n",
       " 0.018894560581009138,\n",
       " -0.006978390718394216,\n",
       " 0.0038229992969434305,\n",
       " -0.012092518585581333,\n",
       " 0.023794548517905263,\n",
       " -0.025091975855533688,\n",
       " 0.008798566621690356,\n",
       " -0.004613421844152312,\n",
       " -0.009988924459834571,\n",
       " 0.0023161580254153716,\n",
       " -0.00039186529353789834,\n",
       " -0.021791726288411185,\n",
       " -0.012306656653227182,\n",
       " 0.020116407366633546,\n",
       " 0.0330528839476578,\n",
       " 0.00540384415853055,\n",
       " -0.012734933719841449,\n",
       " -0.01333955950942213,\n",
       " 0.020695840880642986,\n",
       " 0.010952546229902174,\n",
       " 0.002412205555091199,\n",
       " -0.019599957335519932,\n",
       " -0.02528092071628572,\n",
       " 0.018957542201259815,\n",
       " -0.012306656653227182,\n",
       " -0.03176805367913756,\n",
       " -0.011872081983381388,\n",
       " -0.0019603106137135694,\n",
       " 0.011128895884191158,\n",
       " 0.029248779555884726,\n",
       " -0.002158703509127391,\n",
       " -0.00990074963269008,\n",
       " -0.014372462365617079,\n",
       " 0.027157781567923586,\n",
       " -0.004355195897272931,\n",
       " -0.010971440902241891,\n",
       " 0.012949071787487297,\n",
       " 0.021716147599052313,\n",
       " 0.0021964926209761836,\n",
       " 0.007967205695355634,\n",
       " 0.005649473501963024,\n",
       " 0.001711532235834691,\n",
       " 0.009844065615670925,\n",
       " 0.012678249702822296,\n",
       " -0.010045607545531153,\n",
       " 0.0010187317355248393,\n",
       " 0.01991486543677332,\n",
       " 0.019146487062011847,\n",
       " -0.023668585277403904,\n",
       " 0.010133782372675645,\n",
       " 0.0031679877920670497,\n",
       " -0.037587579232158046,\n",
       " 0.021476816324512652,\n",
       " 0.0076963840763519175,\n",
       " -0.02995417631039552,\n",
       " -0.0014863720819651384,\n",
       " 0.016879139419761723,\n",
       " 0.019209468682262525,\n",
       " -0.01417091950443428,\n",
       " 0.018138777412710715,\n",
       " 0.003989901382231913,\n",
       " 0.020003041195240386,\n",
       " 0.004096970416054837,\n",
       " 0.0050605931174450116,\n",
       " 0.0071862311828085395,\n",
       " -0.003778911883371184,\n",
       " 0.004742534351930716,\n",
       " -0.0031868824644067676,\n",
       " -0.005756542535785948,\n",
       " 0.029475513761316196,\n",
       " 0.0008990661564626688,\n",
       " 0.010631338662772116,\n",
       " -0.008956021603639622,\n",
       " 0.006373764928813535,\n",
       " -0.003530133505492306,\n",
       " 0.01002041526995991,\n",
       " -0.014410250778973942,\n",
       " 0.010070800752424967,\n",
       " 0.01640047873332754,\n",
       " -0.012533391789981222,\n",
       " -0.006272993963883422,\n",
       " -0.007803453110174842,\n",
       " 0.006953197977161688,\n",
       " 0.006178521067846119,\n",
       " -0.017408188382628677,\n",
       " -0.021237485049972988,\n",
       " -0.008256922452360352,\n",
       " -0.002988489568992946,\n",
       " -0.010114887700335928,\n",
       " 0.01218699101595735,\n",
       " -0.01186578344882729,\n",
       " -0.03191921105785531,\n",
       " 0.02796394928736449,\n",
       " -0.006433597747448451,\n",
       " -0.00480551643784268,\n",
       " 0.0077341734210313534,\n",
       " -0.025356499405644592,\n",
       " -0.0435078738874635,\n",
       " -0.03204517429835666,\n",
       " 0.007072863614431519,\n",
       " 0.014851123983373833,\n",
       " -0.00889933758662047,\n",
       " -0.0006136795837562302,\n",
       " -0.005063741919060774,\n",
       " -0.025003800097066622,\n",
       " -0.02444956072127357,\n",
       " 0.008338798744950748,\n",
       " 0.0009541752488049942,\n",
       " -0.02361820072626142,\n",
       " 0.010694321214345366,\n",
       " -0.0074066673193471984,\n",
       " -0.015972200735390702,\n",
       " -0.0006928005585175124,\n",
       " -0.0061848191367389295,\n",
       " 0.015468346842062707,\n",
       " -0.014347269158723265,\n",
       " -0.011733521673771839,\n",
       " 0.004292214277022253,\n",
       " -0.015581713944778442,\n",
       " 0.005120425936079928,\n",
       " -0.019851883816522642,\n",
       " -0.022648278558994577,\n",
       " 0.016803562593047997,\n",
       " 0.014284287538472587,\n",
       " -0.007904224075104955,\n",
       " -0.001998099725562362,\n",
       " 0.029022043487808114,\n",
       " -0.010543164766950195,\n",
       " 0.0032687587569971634,\n",
       " 0.012898686305022241,\n",
       " 0.004465414198372904,\n",
       " -0.022698664972782206,\n",
       " 8.192563085883627e-05,\n",
       " 0.003191606132491697,\n",
       " -0.009705505306061377,\n",
       " 0.004591377904535546,\n",
       " -0.003939515899766857,\n",
       " 0.01362927626642685,\n",
       " 0.014939298810518324,\n",
       " 0.034438483318463,\n",
       " 0.0051078293326330204,\n",
       " 0.02541948102589527,\n",
       " -0.02211923145877277,\n",
       " 0.0027475838936454025,\n",
       " -0.017231838728339693,\n",
       " -0.0011832718703555358,\n",
       " -0.018138777412710715,\n",
       " 0.013150613717347525,\n",
       " 0.01968813123134185,\n",
       " 0.009182755809071092,\n",
       " -0.008061679057054221,\n",
       " -0.004194592113707902,\n",
       " 0.024185037171162667,\n",
       " 0.01288609016723662,\n",
       " 0.03431252007796164,\n",
       " -0.001850092312613597,\n",
       " -0.010921055419776836,\n",
       " -0.025684004576006177,\n",
       " 0.002594852812611708,\n",
       " -0.0013730047464187605,\n",
       " 0.007885329402765238,\n",
       " -0.009453577893736093,\n",
       " -0.004720490877975236,\n",
       " 0.018881963511900943,\n",
       " 0.036126397446703686,\n",
       " 0.005044847712382342,\n",
       " 0.040132045630982126,\n",
       " 0.00026885385082450704,\n",
       " -0.027938755149148106,\n",
       " -0.03327962001573441,\n",
       " 0.008401781296523999,\n",
       " 0.011053317194832287,\n",
       " 0.006928005235929159,\n",
       " -0.015707677185279798,\n",
       " -0.003782061150648233,\n",
       " 0.008149853884198715,\n",
       " -0.025129765200213122,\n",
       " 0.030004560861538004,\n",
       " 0.003819850262497025,\n",
       " -0.007576718439082086,\n",
       " 0.009692909168275756,\n",
       " 0.028845695696164273,\n",
       " -0.02140123763515378,\n",
       " 0.01211141325792105,\n",
       " 0.03302768980944141,\n",
       " 0.006569008789780952,\n",
       " -0.011198176038995932,\n",
       " -0.0021744489141900603,\n",
       " 0.030130524102039362,\n",
       " -0.0009022152491167349,\n",
       " -0.00047393853174843107,\n",
       " -0.01521641849841485,\n",
       " 0.00582897195786777,\n",
       " 0.0030782387969453193,\n",
       " -0.03657986958285691,\n",
       " -0.008376588089630184,\n",
       " -0.015997394873607088,\n",
       " 0.027989141562935735,\n",
       " 0.030987078235267897,\n",
       " 0.015921816184248216,\n",
       " 0.008483657589114393,\n",
       " -9.225859658695868e-05,\n",
       " -0.008993810948319058,\n",
       " 0.009081984844140977,\n",
       " -0.0191338918555488,\n",
       " 0.008294711797039788,\n",
       " -0.011758714880665653,\n",
       " -0.001968183316244904,\n",
       " -0.020771419570001858,\n",
       " -0.004833858446352256,\n",
       " -0.0041756979070294705,\n",
       " 0.018378108687250375,\n",
       " -0.015896623908676976,\n",
       " 0.013654468541998092,\n",
       " -0.005586491416051059,\n",
       " -0.001137609939560087,\n",
       " 0.00060344498897477,\n",
       " -0.0039237704947041865,\n",
       " -0.00284992949221404,\n",
       " -0.0188567712363297,\n",
       " -0.015997394873607088,\n",
       " -0.0019555869456286397,\n",
       " 0.017244435797447884,\n",
       " -0.003782061150648233,\n",
       " -0.01991486543677332,\n",
       " -0.031944401470781404,\n",
       " -9.088086491282598e-05,\n",
       " 0.0018941796097705216,\n",
       " -0.0020815506517912817,\n",
       " -0.02587295129940335,\n",
       " 0.007797155041282031,\n",
       " 0.02384493493169289,\n",
       " -0.025394288750324026,\n",
       " 0.004503203543052339,\n",
       " 0.004515799680837961,\n",
       " 0.01646346035357822,\n",
       " 0.004254424932342818,\n",
       " -0.00826322098691445,\n",
       " -0.007457052801812255,\n",
       " 0.023580411381581984,\n",
       " 0.015140840740378551,\n",
       " 0.003634053737699468,\n",
       " -0.01802540937867241,\n",
       " 0.01594700845981946,\n",
       " 0.00011356417930310837,\n",
       " -0.012401129083603198,\n",
       " -0.021879900184233105,\n",
       " -0.0007030350950913119,\n",
       " -0.0005475485798131825,\n",
       " -0.0040780757437151195,\n",
       " 0.005303073193600437,\n",
       " -0.0050605931174450116,\n",
       " 0.0055487025370329094,\n",
       " 0.022081442114093332,\n",
       " -0.00408437381260793,\n",
       " -0.005652622303578786,\n",
       " 0.011708329398200597,\n",
       " 0.023152133383645145,\n",
       " -0.004093821148777789,\n",
       " -0.011563470554036951,\n",
       " -0.005750244466893137,\n",
       " -0.026931045499846974,\n",
       " -0.013553697577067978,\n",
       " 0.06741578857676192,\n",
       " 0.003561624548448288,\n",
       " -0.007601911180314615,\n",
       " 0.01690433355797811,\n",
       " -0.01218699101595735,\n",
       " -0.003901726555087421,\n",
       " -0.043936151885400336,\n",
       " -0.025822564885615722,\n",
       " -0.005633728096900353,\n",
       " 0.0032813551276134273,\n",
       " -0.001328130132442574,\n",
       " -0.004808665705119728,\n",
       " -0.0009675589362405204,\n",
       " 0.026779889983774376,\n",
       " 0.02481485523631459,\n",
       " 0.0008549788593057441,\n",
       " 0.0042607234668969145,\n",
       " -0.02395830110308605,\n",
       " -0.011771311018451275,\n",
       " 0.0018280487222427954,\n",
       " 0.002757031229815261,\n",
       " 0.0036277554359760143,\n",
       " -0.0019618852473520933,\n",
       " 0.015468346842062707,\n",
       " 0.02305136241871503,\n",
       " 0.003668693815101855,\n",
       " 0.005095233194847399,\n",
       " 0.018655229306469474,\n",
       " -0.002083125285429806,\n",
       " -0.0017823867914473466,\n",
       " 0.013654468541998092,\n",
       " -0.006820936202106235,\n",
       " 0.017282223279482176,\n",
       " 0.009894451098135982,\n",
       " 0.019562167990840495,\n",
       " 0.011191877504441837,\n",
       " 0.00801759117782069,\n",
       " 0.03534542386547916,\n",
       " 0.01017157171735508,\n",
       " -0.007690086007459107,\n",
       " 0.019713323506913093,\n",
       " 0.01035421897487559,\n",
       " -0.006814638133213425,\n",
       " -0.02191768952891254,\n",
       " 0.012407427618157295,\n",
       " 0.007841242454854278,\n",
       " -0.007948311954338488,\n",
       " 0.01647605556004127,\n",
       " 0.0016989358652184268,\n",
       " -0.030659573064906315,\n",
       " 0.023693777552975148,\n",
       " -0.004736236283037905,\n",
       " -0.015266803980879907,\n",
       " -0.023517429761331306,\n",
       " -0.010738408162256326,\n",
       " 0.01311282437266809,\n",
       " -0.006940601839376066,\n",
       " -0.015140840740378551,\n",
       " -0.008830057431815695,\n",
       " -0.01258377727244628,\n",
       " -0.007287002147738653,\n",
       " -0.009774785460866152,\n",
       " -0.01779867517324094,\n",
       " -0.0031553914214507857,\n",
       " -0.026704311294415504,\n",
       " -0.025356499405644592,\n",
       " -0.025898143574974594,\n",
       " 0.008748181139225299,\n",
       " -0.015468346842062707,\n",
       " -0.0006652459832425193,\n",
       " -0.01312542051045371,\n",
       " -0.04045955080134371,\n",
       " -0.015191226222843609,\n",
       " 0.007872733264979616,\n",
       " 0.019285047371621396,\n",
       " 0.0013037245916139861,\n",
       " -0.0071484418381291045,\n",
       " -0.007368878440329049,\n",
       " 0.0021775979486364657,\n",
       " 0.015934411390711268,\n",
       " 0.0007179932997500408,\n",
       " -0.03106265692462677,\n",
       " 0.0018752850538461252,\n",
       " 0.0027082201481580855,\n",
       " 0.014221304986899337,\n",
       " 0.008143555349644617,\n",
       " 0.0032467152830416833,\n",
       " 0.00830730793482541,\n",
       " -0.012785319202306506,\n",
       " 0.027006624189205842,\n",
       " 0.012709740512947635,\n",
       " 0.0021350851687027437,\n",
       " 0.02831664673329732,\n",
       " -0.010448691405251606,\n",
       " 0.025104571061996737,\n",
       " 0.001584781212852787,\n",
       " 0.011890976655721107,\n",
       " -0.005646324234685975,\n",
       " -0.018667824512932522,\n",
       " 0.0038229992969434305,\n",
       " -0.002051634242473824,\n",
       " -0.010014116735405814,\n",
       " -0.00891193372440609,\n",
       " 0.0071295471657893865,\n",
       " 0.0022374307672713813,\n",
       " 0.014536214950797871,\n",
       " 0.013452926612137865,\n",
       " 0.02122488984350994,\n",
       " -0.023517429761331306,\n",
       " -0.005265283848921001,\n",
       " 0.03078553630540767,\n",
       " -0.028744924731234158,\n",
       " 0.02084699825936073,\n",
       " -0.013200999199812581,\n",
       " -0.0044937562068824806,\n",
       " 0.018768595477862637,\n",
       " 0.016979910384691838,\n",
       " 0.031994789747214175,\n",
       " -0.0019933760574774324,\n",
       " -0.027535671289427653,\n",
       " -0.009327614653234737,\n",
       " -0.027611249978786525,\n",
       " 0.0022484527370797646,\n",
       " 0.01423390205600753,\n",
       " 0.0177608858285615,\n",
       " -0.005718753656767797,\n",
       " 0.00752633295661703,\n",
       " -0.02184211083955367,\n",
       " -0.009252035963875867,\n",
       " -0.01802540937867241,\n",
       " -0.008067976660285747,\n",
       " 0.019511781577052866,\n",
       " -0.0009281952489608646,\n",
       " -0.008074275194839843,\n",
       " -0.024928223270352896,\n",
       " -0.011084808936280199,\n",
       " -0.006676077823603876,\n",
       " 0.0006404468422119612,\n",
       " -0.0076585947316724825,\n",
       " -0.013742643369142583,\n",
       " -0.022257791768382315,\n",
       " -0.005904550181565355,\n",
       " -0.002062656212282207,\n",
       " -0.008552937743919168,\n",
       " -0.04360864298974847,\n",
       " -0.04917624206176624,\n",
       " -0.013578890783961793,\n",
       " 0.008521446933793829,\n",
       " -0.00917015967128547,\n",
       " 0.017093278418730144,\n",
       " -0.00707916168332433,\n",
       " 0.000594785027831834,\n",
       " -0.015909219115140025,\n",
       " -0.010694321214345366,\n",
       " -0.011254859124692514,\n",
       " -0.013818221127178884,\n",
       " 0.004141057829627083,\n",
       " -0.011185579901210312,\n",
       " 0.023303290762362885,\n",
       " 0.02428580813609278,\n",
       " 0.031490933059918465,\n",
       " 0.00629818670511595,\n",
       " -0.0017367249770672193,\n",
       " 0.01393158916121719,\n",
       " -0.00748854407759888,\n",
       " -0.01826474065321207,\n",
       " 0.0005156640530705517,\n",
       " -0.018315127066999698,\n",
       " -0.014070148539504166,\n",
       " 0.012061027775455995,\n",
       " 0.020506896019890954,\n",
       " 0.03393463035645757,\n",
       " 0.0032687587569971634,\n",
       " 0.016677597489901497,\n",
       " 0.002191768952891254,\n",
       " -0.00016217830078823174,\n",
       " -0.015921816184248216,\n",
       " -0.0020469105743888944,\n",
       " -0.017685309001847775,\n",
       " -0.012665653565036674,\n",
       " -0.01661461586965082,\n",
       " 0.0050291018416583864,\n",
       " 0.0008982788978510674,\n",
       " 0.008956021603639622,\n",
       " 0.016438266215361833,\n",
       " -0.002338201732201495,\n",
       " 0.019499186370589817,\n",
       " 0.01091475781654531,\n",
       " 0.017370399037949243,\n",
       " 0.005810077285528052,\n",
       " 0.030281681480757102,\n",
       " -0.021816918563982428,\n",
       " 0.027308937083996183,\n",
       " 0.004455966862203045,\n",
       " 0.010675426542005647,\n",
       " -0.012722336650733256,\n",
       " 0.009793680133205869,\n",
       " -0.0025476165974236994,\n",
       " 0.010864372334080254,\n",
       " 0.015997394873607088,\n",
       " 0.014636985915727985,\n",
       " 0.021476816324512652,\n",
       " -0.01753415162313003,\n",
       " -0.01521641849841485,\n",
       " -0.002679878372479152,\n",
       " 0.030004560861538004,\n",
       " 0.008301010331593885,\n",
       " 0.00995743271838666,\n",
       " 0.004238679527280149,\n",
       " -0.011525681209357515,\n",
       " 0.00036096482550785405,\n",
       " -0.013994570781467867,\n",
       " 0.0033789770580971356,\n",
       " -0.011683136191306783,\n",
       " 0.005671516975918504,\n",
       " -0.003816700995219977,\n",
       " -0.019952654781452757,\n",
       " 0.02846780411201506,\n",
       " -0.014322076883152023,\n",
       " -0.02563362002486369,\n",
       " -0.008206536969895296,\n",
       " -0.0001641464909729806,\n",
       " 0.04401172684946892,\n",
       " 0.007394071181561577,\n",
       " 0.006376914196090584,\n",
       " 0.020229775400671855,\n",
       " 0.0033978717304368536,\n",
       " -0.009283526774001205,\n",
       " 0.028291454457726076,\n",
       " -0.025532849059933575,\n",
       " -0.004755130955377623,\n",
       " 0.03317884718815915,\n",
       " 0.001355684707717567,\n",
       " 0.0006530432128282254,\n",
       " -0.021716147599052313,\n",
       " 0.0033569335841416555,\n",
       " 0.00466065805934032,\n",
       " -0.01476294915622934,\n",
       " -0.027157781567923586,\n",
       " 0.01861743996179004,\n",
       " 0.023202519797432774,\n",
       " -0.00240275821892134,\n",
       " -0.020897382810503212,\n",
       " 0.014498425606118436,\n",
       " -0.037083722544862335,\n",
       " 0.018630035168253088,\n",
       " 0.0004782685414237295,\n",
       " -0.01743338065819992,\n",
       " -0.02103594312011276,\n",
       " 0.005753393268508899,\n",
       " -0.01534238267023878,\n",
       " 0.008735585001439677,\n",
       " -0.02635161384848268,\n",
       " 0.018516668996859924,\n",
       " 0.027334129359567427,\n",
       " -0.004002497520017534,\n",
       " -0.00883635596636979,\n",
       " -0.0035490281778320235,\n",
       " 0.0009030025077283361,\n",
       " 0.000851042449832416,\n",
       " -0.017723096483882067,\n",
       " 0.025293517785393915,\n",
       " -0.0055770440798812,\n",
       " 0.011380823296516442,\n",
       " 0.01100293171236723,\n",
       " 0.001711532235834691,\n",
       " -0.027334129359567427,\n",
       " -0.004471712732927,\n",
       " -0.0003526984645668883,\n",
       " 0.017836464517920373,\n",
       " -0.022207405354594686,\n",
       " 0.0010848626230525655,\n",
       " -0.006449343152511121,\n",
       " 0.009831469477885305,\n",
       " 0.0009549625656242563,\n",
       " 0.014196112711328094,\n",
       " -0.007702682145244728,\n",
       " -0.004430774120970517,\n",
       " -0.004323705087147593,\n",
       " -0.0010730535692555633,\n",
       " 0.02145162404894141,\n",
       " -0.00988185496035036,\n",
       " -0.0049346294112823695,\n",
       " -0.013553697577067978,\n",
       " -0.012489303910747691,\n",
       " -0.029324356382598452,\n",
       " -0.035446192967764134,\n",
       " 0.0064367465490642135,\n",
       " 0.005422738830870268,\n",
       " -0.02846780411201506,\n",
       " -0.008250624849128828,\n",
       " -0.010480182215376945,\n",
       " -0.0014438593020314162,\n",
       " 0.024777065891635152,\n",
       " 0.010876968471865875,\n",
       " -0.010341622837089968,\n",
       " 0.008351395814058941,\n",
       " 0.02609968550483482,\n",
       " -0.02227038883749051,\n",
       " 0.0010037735308661103,\n",
       " 0.009497665772969625,\n",
       " 0.014649582053513606,\n",
       " -0.020859593465823778,\n",
       " 0.02222000242370288,\n",
       " -0.003093984202007989,\n",
       " -0.01534238267023878,\n",
       " 0.003945813968659667,\n",
       " -0.015581713944778442,\n",
       " -0.005277880452367908,\n",
       " 0.013075035027988654,\n",
       " 0.010593550249415253,\n",
       " 0.005296775124707626,\n",
       " -0.0032042025031079613,\n",
       " 0.004843305316860829,\n",
       " 0.027384515773355055,\n",
       " 0.025923335850545837,\n",
       " 0.006827234270999046,\n",
       " -0.011727223139217743,\n",
       " -0.00084238248868948,\n",
       " 0.03154131761106095,\n",
       " -0.0053408620726185865,\n",
       " 0.0023759908440502873,\n",
       " -0.003971006709892195,\n",
       " -0.005696709717151032,\n",
       " 0.005838418828376343,\n",
       " -0.009415789480379229,\n",
       " -0.0018957541269937243,\n",
       " 0.0003822212154747151,\n",
       " -0.038998372741179635,\n",
       " -0.004166250570859612,\n",
       " 0.001213975480076934,\n",
       " 0.01312542051045371,\n",
       " 0.005161364082375126,\n",
       " -0.017622325518951952,\n",
       " -0.034690413524756,\n",
       " -0.012961667925272919,\n",
       " -0.033556736909663216,\n",
       " 0.002533445593168911,\n",
       " 0.005369204081128163,\n",
       " -0.020141601504849935,\n",
       " 0.014359865296508886,\n",
       " 0.00555500060592572,\n",
       " 0.011758714880665653,\n",
       " 0.0017729395716928093,\n",
       " 0.008420675968863716,\n",
       " -0.015543924600099006,\n",
       " -0.022182213079023447,\n",
       " -0.007948311954338488,\n",
       " -0.01917167933758309,\n",
       " -0.012590074875677804,\n",
       " -0.004430774120970517,\n",
       " 0.02841741769822743,\n",
       " 0.007419263922794106,\n",
       " -0.014586600433262927,\n",
       " -0.014246498193793151,\n",
       " -0.005901400914288307,\n",
       " -0.031944401470781404,\n",
       " -0.022169617872560398,\n",
       " -0.013389944991887188,\n",
       " 0.00336952995475792,\n",
       " -0.01006450221787087,\n",
       " 0.015972200735390702,\n",
       " -0.007771962300049503,\n",
       " 0.0344132929055369,\n",
       " 0.001829623239465998,\n",
       " 0.0034734499541344385,\n",
       " 0.004714192809082425,\n",
       " -0.027837984184217995,\n",
       " 0.009466174962844286,\n",
       " -0.0016186339734359121,\n",
       " 0.025066783579962445,\n",
       " -0.006005321146495469,\n",
       " -0.021690955323481073,\n",
       " -0.005136171341142597,\n",
       " 0.004689000067849897,\n",
       " 0.00842697357209524,\n",
       " -0.007387773112668767,\n",
       " 0.00842697357209524,\n",
       " -0.013276577889171453,\n",
       " -0.0031333479474953056,\n",
       " -0.01288609016723662,\n",
       " 0.0027066457473502044,\n",
       " -0.0038859811500247515,\n",
       " 0.005507764390737712,\n",
       " 0.02056987764014163,\n",
       " -0.01408274560861236,\n",
       " -0.002805842078641794,\n",
       " 0.0007723151334807648,\n",
       " -0.006764252650748368,\n",
       " 0.00482755991179816,\n",
       " -2.288013086217211e-06,\n",
       " 0.012369638273477859,\n",
       " -0.012438918428282634,\n",
       " 0.02390791655194357,\n",
       " -0.014070148539504166,\n",
       " -0.006650885082371347,\n",
       " 0.00523694230607271,\n",
       " -0.02087219053493197,\n",
       " 0.022673472697210963,\n",
       " -0.021816918563982428,\n",
       " 0.010058203683316773,\n",
       " 0.024651102651133797,\n",
       " 0.01017157171735508,\n",
       " 0.0027129438162430152,\n",
       " -0.0011793354026745468,\n",
       " 0.001222635499427531,\n",
       " -0.0053408620726185865,\n",
       " 0.004468563465649952,\n",
       " -0.0004381175955324722,\n",
       " -0.0095669459277744,\n",
       " 0.0067831468574268,\n",
       " -0.005019654971149814,\n",
       " -0.017118470694301384,\n",
       " -0.03728526447472256,\n",
       " 0.008571832416258886,\n",
       " -0.022711260179245255,\n",
       " -0.020834401190252535,\n",
       " 0.0024531437013863965,\n",
       " 0.02831664673329732,\n",
       " 0.018554458341539362,\n",
       " -0.03201998016014027,\n",
       " -0.014662178191299226,\n",
       " -0.019977848919669142,\n",
       " 0.01840330096282162,\n",
       " 0.006518623307315895,\n",
       " -0.011242262986906892,\n",
       " 0.005485720451120946,\n",
       " -0.006093495507978675,\n",
       " -0.009289825308555301,\n",
       " 0.016362689388648106,\n",
       " -0.025684004576006177,\n",
       " -0.006232055817588224,\n",
       " 0.009415789480379229,\n",
       " 0.016828754868619237,\n",
       " -0.0015721848422365228,\n",
       " 0.03957780439254393,\n",
       " 0.24023802509687397,\n",
       " -0.017458572933771163,\n",
       " -0.009094581913249172,\n",
       " 0.020909979879611407,\n",
       " 0.02407166913712436,\n",
       " 0.021287871463760617,\n",
       " 0.007217721992933879,\n",
       " -0.0014745629117528147,\n",
       " 0.002662558333777958,\n",
       " 0.008313606469379505,\n",
       " -0.008540340674810975,\n",
       " 0.0017036595333033563,\n",
       " -0.008282115659254166,\n",
       " 0.009516560445309344,\n",
       " 0.002286241848928557,\n",
       " 0.006795743460873707,\n",
       " -0.024436963652165376,\n",
       " -0.027485286738285167,\n",
       " -0.02683027453491686,\n",
       " -0.02502899423528301,\n",
       " 0.005958084465646174,\n",
       " -0.030054947275325632,\n",
       " -0.02363079593272447,\n",
       " -0.012848300822557184,\n",
       " 0.015833640425781153,\n",
       " -0.01159496136416229,\n",
       " -0.008956021603639622,\n",
       " -0.005044847712382342,\n",
       " 0.03227190664114299,\n",
       " 0.01370485402446315,\n",
       " 0.004500054275775291,\n",
       " -0.01699250745380003,\n",
       " 0.016135953320571494,\n",
       " 0.008408078899755523,\n",
       " -0.02184211083955367,\n",
       " -0.01452361881301225,\n",
       " 0.030054947275325632,\n",
       " 0.0064745358937436485,\n",
       " 0.029777826656106537,\n",
       " 0.010001520597620193,\n",
       " -0.01505266591323406,\n",
       " -0.009453577893736093,\n",
       " -0.00856553388170479,\n",
       " -0.012394831480371673,\n",
       " -0.00044992670753713507,\n",
       " 0.010121186234890023,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e416f8-3074-415e-97a7-9417b176e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db_without_split'\n",
    "\n",
    "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=docs, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dd44f62-f716-446a-b15a-fb1beb237a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d54d20-985f-4ec1-a1ba-bdaf00ce912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cef8b3e-7d68-484a-8047-5cd48c8562e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 1}) # by default search_type=\"similarity_score_threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dde10be-d471-447c-8950-3267b10336d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"which candidate is good fit for Machine learning engineer roles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61180fa-2316-4465-bf09-952f6e0e1948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\n",
      "\n",
      "SUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\n",
      "\n",
      "TECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\n",
      "\n",
      "Programming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\n",
      "\n",
      "EDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\n",
      "\n",
      "Visvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\n",
      "\n",
      "July 2018\n",
      "\n",
      "WORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\n",
      "\n",
      "September 2023 - Present\n",
      "\n",
      "Engineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \n",
      "\n",
      "May 2021 – June 2022\n",
      "\n",
      "Developed a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\n",
      "\n",
      "AI Engineer ( Telerad Tech Pvt Ltd., India ) \n",
      "\n",
      "August 2018 – May 2021\n",
      "\n",
      "Led a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \n",
      "\n",
      "July 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\n",
      "\n",
      "ACADEMIC PROJECT \n",
      "\n",
      "End-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf5acd49-18af-4721-b627-95c1239ce5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = retriever.invoke(\"which candidate is good fit for Data Analyst roles.\")\n",
    "docs = retriever.invoke(\"Give name of candidate who is good fit for Data Analyst roles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f541709d-52ce-4cf2-b9b3-b6a826c956c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOHAMMED WASIM R D\n",
      "\n",
      "(312) 479-1954 • wasimwassu44@gmail.com • LinkedIn • GitHub • Tableau\n",
      "\n",
      "SKILLS Technical Skills: Statistical Analysis, Data Analysis, Exploratory Data Analysis, Predictive Modeling, Data Visualization, Extract Transform Load (ETL) Programming Languages: Python, SQL, R, SAS Tools: Excel, Power BI, Tableau, Salesforce, PySpark, Azure Data Studio. Cloud: AWS (EC2, S3, EMR), Microsoft Azure Libraries: NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn, ggplot, plotly, dplyr, caret.\n",
      "\n",
      "EXPERIENCE\n",
      "\n",
      "LABELMASTER, Chicago, IL Data Scientist (Python | Power BI | Data Visualization | Statistical Modeling) January 2023 – May 2023  Conducted advanced statistical analysis on a 1.1M-record dataset using Python, statistical modeling, and\n",
      "\n",
      "\n",
      "\n",
      "machine. learning algorithms to derive actionable metrics. Implemented data-driven strategies, resulting in a remarkable 20% surge in click-through rates and a 15% increase in conversions over five months.\n",
      "\n",
      "Collaborated with cross-functional teams to ensure alignment and communication to understand trend detection.\n",
      "\n",
      "Obtained a remarkable MAE of 0.004 and selected Random Forest Regressor with an R2 score of 0.96.\n",
      "\n",
      "SOUTHEAST TOYOTA DISTRIBUTORS, Jacksonville, FL June 2022 – August 2022 Data Analyst (SQL | Power BI | Data Visualization | Reporting & Analytics)  Automated Power BI reporting, reducing time by 25% and enhancing data-driven decision-making with\n",
      "\n",
      "monthly and quarterly reports for internal and external stakeholders.\n",
      "\n",
      "Engineered efficient ETL processes, achieving a remarkable 15% decrease in error rates and ensuring reliable data accuracy and integrity.\n",
      "\n",
      "Leveraged Python to achieve 4x accuracy improvement and collaborated on ad hoc reporting to inform strategic risk management.\n",
      "\n",
      "BYJUS THINK AND LEARN, Bangalore, India Data Analyst – Operations (Power BI | Data Analysis| Data Visualization) August 2019 – August 2021  Achieved a 30% reduction in processing time by optimizing logistics and supply chain processes using\n",
      "\n",
      "Python and Tableau.\n",
      "\n",
      "Delivered a remarkable 25% improvement in on-time delivery rates and reduced asset loss through data driven solutions.\n",
      "\n",
      "Streamlined operations with data insights, resulting in 8% improvement in customer satisfaction through enhanced order fulfillment and reduced delivery time.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "ILLINOIS INSTITUTE OF TECHNOLOGY, Chicago, IL Master of Science in Data Science May 2023 Courses – Machine Learning, Natural Language Processing, Statistics, Time Series.\n",
      "\n",
      "PRESIDENCY UNIVERSITY, India\n",
      "\n",
      "Bachelor of Technology in Computer Science\n",
      "\n",
      "PROJECTS\n",
      "\n",
      "Risk Analytics in Financial Services - Built predictive models for financial customer data achieving 82.65% accuracy with the Random Forest model. Enabled more profitable customer behavior predictions.\n",
      "\n",
      "Airbnb Statistical Analysis - Performed statistical analysis, evaluated multiple models (decision trees, linear regression), and through feature selection and logarithmic transformation, gained a significantly improved R- Square value of 0.5467 for price prediction in NYC Airbnb data.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e405bd76-420a-4504-82f0-2333a5697ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c2d8483-d80e-4421-82b0-43dc179efbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d7243b-4f1d-429f-89f4-47475d7e19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b671f6c5-b79a-4d4f-97ab-ae57f2922cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given job description, the following candidates seem to be a good fit based on their skills, education, and work experience:\n",
      "\n",
      "1. Naveen Raju S G - The candidate has a Master of Science in Artificial Intelligence, experience in Machine Learning, Deep Learning/Computer Vision, and Data Science. They have worked on projects involving Deep Learning algorithms and image processing pipelines. They also have experience with Python, TensorFlow, and other relevant libraries. They have experience working with Product Management and have knowledge of ML Ops and ML services. They have good communication and collaboration skills.\n",
      "\n",
      "Please note that I cannot provide the candidate's name without a subheading as it is not mentioned in the provided context.\n",
      "\n",
      "\n",
      "Sources:\n",
      "E:\\LangChain course\\Lang Chain for LLM application development\\cv_read\\CV_ML_NAVEEN RAJU SREERAMA RAJU GOVINDA RAJU.pdf\n"
     ]
    }
   ],
   "source": [
    "# full example\n",
    "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
    "job_description = \"MS or PhD in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
    "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
    "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
    "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
    "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
    "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
    "Strong communication and collaboration skills\"\n",
    "question = warning+job_description + \" Based on the given job description\"\n",
    "query = question + \"short list resumes which is good fit based on skills,education and work experience mwntioned in it? also provide the candidate name which will be mentioned in first line of pdf without subheading\"\n",
    "# query = \"short list resumes which is good fit for Data analysis roles based on skills,education and work experience mwntioned in it?\"\n",
    "\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b9bd9-69c0-426a-a1c9-b1b7d5e43f9a",
   "metadata": {},
   "source": [
    "## Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "975f8731-6dc1-4848-bd47-937763a21e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_2 = vectordb.as_retriever(search_kwargs={\"k\": 1}) # by default search_type=\"similarity_score_threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ccaec9d-6172-4db2-bb9f-cafec2bd3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
    "job_description = \"MS or PhD in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
    "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
    "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
    "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
    "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
    "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
    "Strong communication and collaboration skills\"\n",
    "question = warning+job_description + \" Based on the given job description\"\n",
    "query = question + \"retrive the full document information of a resume which is good fit based on skills,education and work experience mwntioned in it? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca646022-f1a3-4f70-bbef-5b9b708faed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_doc = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f55cf749-03b8-408a-b0d3-cc458522351a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\\n\\nSUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\\n\\nTECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\\n\\nProgramming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\\n\\nEDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\\n\\nVisvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\\n\\nJuly 2018\\n\\nWORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\\n\\nSeptember 2023 - Present\\n\\nEngineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \\uf06c\\n\\nMay 2021 – June 2022\\n\\nDeveloped a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\\n\\nAI Engineer ( Telerad Tech Pvt Ltd., India ) \\uf06c\\n\\nAugust 2018 – May 2021\\n\\nLed a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nINTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \\uf06c\\n\\nJuly 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\\n\\nACADEMIC PROJECT \\uf06c\\n\\nEnd-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers\\' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nCERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark', metadata={'source': 'E:\\\\LangChain course\\\\Lang Chain for LLM application development\\\\cv_read\\\\CV_ML_NAVEEN RAJU SREERAMA RAJU GOVINDA RAJU.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "print(resume_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97c60e0a-35da-4fe6-bda5-2f8774a9639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_doc = resume_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dce8a5a-0a88-4d46-9c01-583b5ffa1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\n",
      "\n",
      "SUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\n",
      "\n",
      "TECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\n",
      "\n",
      "Programming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\n",
      "\n",
      "EDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\n",
      "\n",
      "Visvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\n",
      "\n",
      "July 2018\n",
      "\n",
      "WORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\n",
      "\n",
      "September 2023 - Present\n",
      "\n",
      "Engineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \n",
      "\n",
      "May 2021 – June 2022\n",
      "\n",
      "Developed a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\n",
      "\n",
      "AI Engineer ( Telerad Tech Pvt Ltd., India ) \n",
      "\n",
      "August 2018 – May 2021\n",
      "\n",
      "Led a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \n",
      "\n",
      "July 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\n",
      "\n",
      "ACADEMIC PROJECT \n",
      "\n",
      "End-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\n"
     ]
    }
   ],
   "source": [
    "print(resume_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d72965-af4e-4d73-b4e0-2cbf99a515ce",
   "metadata": {},
   "source": [
    "## create output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f135fca-088e-4a66-9103-0b434e848341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_review = \"\"\"\\\n",
    "# This leaf blower is pretty amazing.  It has four settings:\\\n",
    "# candle blower, gentle breeze, windy city, and tornado. \\\n",
    "# It arrived in two days, just in time for my wife's \\\n",
    "# anniversary present. \\\n",
    "# I think my wife liked it so much she was speechless. \\\n",
    "# So far I've been the only one using it, and I've been \\\n",
    "# using it every other morning to clear the leaves on our lawn. \\\n",
    "# It's slightly more expensive than the other leaf blowers \\\n",
    "# out there, but I think it's worth it for the extra features.\n",
    "# \"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Skills: what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\n",
    "\n",
    "Education: What is the highest education of the candidate and what is the GPA as mentioned in the text?\\\n",
    "Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\n",
    "\n",
    "Projects: Extract all project titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Publications: Extract all publication titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "Skills\n",
    "Education\n",
    "Projects\n",
    "Publications\n",
    "Work experience\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dce401c-ff92-40a6-8c29-8d55967dfded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nEducation: What is the highest education of the candidate and what is the GPA as mentioned in the text?Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\\n\\nProjects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\\n\\nPublications: Extract all publication titles mentioned in a textand output them as a comma separated Python list.\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\nSkills\\nEducation\\nProjects\\nPublications\\nWork experience\\n\\ntext: {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ebdd53d-d9f3-49f1-acdd-09321a3b65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "turbo_llm_memory = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "\n",
    "memory_llm_conversation = ConversationChain(\n",
    "    llm=turbo_llm_memory, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95216e8f-9ec0-471a-9613-7ca5b07ed76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: [HumanMessage(content='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nEducation: What is the highest education of the candidate and what is the GPA as mentioned in the text?Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\\n\\nProjects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\\n\\nPublications: Extract all publication titles mentioned in a textand output them as a comma separated Python list.\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\nSkills\\nEducation\\nProjects\\nPublications\\nWork experience\\n\\ntext: NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\\n\\nSUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\\n\\nTECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\\n\\nProgramming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\\n\\nEDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\\n\\nVisvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\\n\\nJuly 2018\\n\\nWORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\\n\\nSeptember 2023 - Present\\n\\nEngineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \\uf06c\\n\\nMay 2021 – June 2022\\n\\nDeveloped a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\\n\\nAI Engineer ( Telerad Tech Pvt Ltd., India ) \\uf06c\\n\\nAugust 2018 – May 2021\\n\\nLed a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nINTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \\uf06c\\n\\nJuly 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\\n\\nACADEMIC PROJECT \\uf06c\\n\\nEnd-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers\\' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nCERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\\n')]\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=resume_doc)\n",
    "# chat = ChatOpenAI(temperature=0.0, model=turbo_llm_memory)\n",
    "response = memory_llm_conversation(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6349644a-8317-40f7-8888-984a1b3de96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eed29491-8039-4721-9c01-739e38537499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'history', 'response'])\n"
     ]
    }
   ],
   "source": [
    "print(response.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51737e9c-e93c-4382-9f02-8834a9836080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can help you with that! Here is the information extracted from the text:\n",
      "\n",
      "Skills: The technical skills mentioned in the text are AWS (Amazon Web Services), Keras, TensorFlow, Numpy, Pandas, Matplotlib, scikit-learn, Apache PySpark, Large language models (LLM), MLOPS, Machine learning, Deep Learning/Computer Vision, NLP, Python, R, C++, SQL, Open CV, scikit-image, Apache Airflow, and Git and GitHub. The non-technical skills are project leadership, Agile methodology, and technical instruction.\n",
      "\n",
      "Education: The highest education of the candidate is a Master of Science in Artificial Intelligence from Illinois Institute of Technology. The GPA mentioned in the text is 3.833/4.\n",
      "\n",
      "Projects: The project titles mentioned in the text are \"Developed a deep learning algorithm for number plate detection and recognition with 90% accuracy\", \"Improved the crowd detection, human trespass detection algorithm by 20%\", \"Led a team of 4+ using the Agile software development method\", \"Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis\", \"Customized and implemented several Deep learning architectures for lung disease segmentation and detection\", \"Customized deep learning architecture for mammogram lesion segmentation\", \"End-to-end automated AWS ML workflow for auto insurance fraud detection\", \"Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization\", \"Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning\", \"Online shoppers' purchasing intentions\", \"Real-time machine learning prediction system for taxi ride fares\", \"Real estate price prediction\", \"TF-IDF algorithm on Wikipedia data using Apache PySpark\", \"Streamlined Real-time Data Streaming and Analytics Pipeline\", and \"Forex data pipeline using Apache Airflow\".\n",
      "\n",
      "Publications: There are no publication titles mentioned in the text.\n",
      "\n",
      "Work experience: The organization names where the candidate has worked are \"Samsung Electro-Mechanics Software India Bangalore Private Limited\", \"Telerad Tech Pvt Ltd., India\", and \"Telerad Tech Pvt Ltd, India\". The number of years or months worked and the designations are not mentioned in the text.\n",
      "\n",
      "Here is the formatted output in JSON format:\n",
      "\n",
      "{\n",
      "  \"Skills\": [\"AWS (Amazon Web Services)\", \"Keras\", \"TensorFlow\", \"Numpy\", \"Pandas\", \"Matplotlib\", \"scikit-learn\", \"Apache PySpark\", \"Large language models (LLM)\", \"MLOPS\", \"Machine learning\", \"Deep Learning/Computer Vision\", \"NLP\", \"Python\", \"R\", \"C++\", \"SQL\", \"Open CV\", \"scikit-image\", \"Apache Airflow\", \"Git and GitHub\"],\n",
      "  \"Education\": [\"Illinois Institute of Technology, Master of Science Artificial Intelligence\", \"GPA: 3.833/4\"],\n",
      "  \"Projects\": [\"Developed a deep learning algorithm for number plate detection and recognition with 90% accuracy\", \"Improved the crowd detection, human trespass detection algorithm by 20%\", \"Led a team of 4+ using the Agile software development method\", \"Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis\", \"Customized and implemented several Deep learning architectures for lung disease segmentation and detection\", \"Customized deep learning architecture for mammogram lesion segmentation\", \"End-to-end automated AWS ML workflow for auto insurance fraud detection\", \"Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization\", \"Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning\", \"Online shoppers' purchasing intentions\", \"Real-time machine learning prediction system for taxi ride fares\", \"Real estate price prediction\", \"TF-IDF algorithm on Wikipedia data using Apache PySpark\", \"Streamlined Real-time Data Streaming and Analytics Pipeline\", \"Forex data pipeline using Apache Airflow\"],\n",
      "  \"Publications\": [],\n",
      "  \"Work experience\": [\"Samsung Electro-Mechanics Software India Bangalore Private Limited\", \"Telerad Tech Pvt Ltd., India\", \"Telerad Tech Pvt Ltd, India\"]\n",
      "}\n",
      "\n",
      "Let me know if there's anything else I can help with!\n"
     ]
    }
   ],
   "source": [
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf98a7-6c85-4e0a-907d-af4d9e36d2d3",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b56c411b-3dd5-4bcc-addf-2742e9977571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7efaf233-2d45-4589-9bca-30bd8eab762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_schema = ResponseSchema(name=\"Skills\",\n",
    "                             description=\"what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\")\n",
    "\n",
    "# Education_schema = ResponseSchema(name=\"Education\",\n",
    "#                                       description=\"What is the highest education of the candidate and what is the GPA as mentioned in the text?\\\n",
    "# Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\")\n",
    "\n",
    "\n",
    "Projects_schema = ResponseSchema(name=\"Projects\",\n",
    "                                    description=\"Extract all project titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\")\n",
    "\n",
    "# Publications_schema = ResponseSchema(name=\"Publications\",\n",
    "#                                     description=\"Extract all publication titles mentioned in a text\\\n",
    "# and output them as a comma separated Python list.\")\n",
    "\n",
    "Work_experience_schema = ResponseSchema(name=\"Work experience\",\n",
    "                                    description=\"Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
    "and output them as a comma separated Python list.\")\n",
    "\n",
    "response_schemas = [skills_schema,\n",
    "                    # Education_schema, \n",
    "                    Projects_schema,\n",
    "                    # Publications_schema,\n",
    "                   Work_experience_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f2026d2-c84b-440f-bb4d-e15d0f6ab59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82f0ae7a-ae16-4a7f-be97-73b829844c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f795494f-bb3e-4b6d-8733-e5534fabb3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Skills\": string  // what are the technical and non technical skills? Answer output them as a comma separated Python list.\n",
      "\t\"Projects\": string  // Extract all project titles mentioned in a textand output them as a comma separated Python list.\n",
      "\t\"Work experience\": string  // Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b021fedb-38d3-4b77-85fb-70fbe06f116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Skills: what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\n",
    "\n",
    "Projects: Extract all project titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "Skills\n",
    "Projects\n",
    "Work experience\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=resume_doc, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8df0e56d-2148-471b-8bce-78f0e5206e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "Skills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\n",
      "\n",
      "Projects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\n",
      "\n",
      "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\n",
      "\n",
      "Format the output as JSON with the following keys:\n",
      "Skills\n",
      "Projects\n",
      "Work experience\n",
      "\n",
      "text: NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\n",
      "\n",
      "SUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\n",
      "\n",
      "TECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\n",
      "\n",
      "Programming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\n",
      "\n",
      "EDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\n",
      "\n",
      "Visvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\n",
      "\n",
      "July 2018\n",
      "\n",
      "WORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\n",
      "\n",
      "September 2023 - Present\n",
      "\n",
      "Engineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \n",
      "\n",
      "May 2021 – June 2022\n",
      "\n",
      "Developed a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\n",
      "\n",
      "AI Engineer ( Telerad Tech Pvt Ltd., India ) \n",
      "\n",
      "August 2018 – May 2021\n",
      "\n",
      "Led a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \n",
      "\n",
      "July 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\n",
      "\n",
      "ACADEMIC PROJECT \n",
      "\n",
      "End-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Skills\": string  // what are the technical and non technical skills? Answer output them as a comma separated Python list.\n",
      "\t\"Projects\": string  // Extract all project titles mentioned in a textand output them as a comma separated Python list.\n",
      "\t\"Work experience\": string  // Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5693cb9-ad99-4496-8cc8-8f12bced479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nProjects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\nSkills\\nProjects\\nWork experience\\n\\ntext: NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\\n\\nSUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\\n\\nTECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\\n\\nProgramming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\\n\\nEDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\\n\\nVisvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\\n\\nJuly 2018\\n\\nWORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\\n\\nSeptember 2023 - Present\\n\\nEngineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \\uf06c\\n\\nMay 2021 – June 2022\\n\\nDeveloped a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\\n\\nAI Engineer ( Telerad Tech Pvt Ltd., India ) \\uf06c\\n\\nAugust 2018 – May 2021\\n\\nLed a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nINTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \\uf06c\\n\\nJuly 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\\n\\nACADEMIC PROJECT \\uf06c\\n\\nEnd-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers\\' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nCERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"Skills\": string  // what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\t\"Projects\": string  // Extract all project titles mentioned in a textand output them as a comma separated Python list.\\n\\t\"Work experience\": string  // Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n}\\n```\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2ba0e7e-6c25-490a-8b97-6f0efe97d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = turbo_llm_memory(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41ea9866-248d-4136-b312-9f8d69713192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"Skills\": \"AWS (Amazon Web Services), Keras, TensorFlow, Numpy, Pandas, Matplotlib, scikit-learn, Apache PySpark, Large language models (LLM), MLOPS, Machine learning, Deep Learning/ComputerVision, NLP, Python, R, C++, SQL, Open CV, scikit-image, Apache Airflow, Git and GitHub\",\n",
      "\t\"Projects\": \"End-to-end automated AWS ML workflow for auto insurance fraud detection, Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization, Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning, Online shoppers' purchasing intentions, Real-time machine learning prediction system for taxi ride fares, Real estate price prediction, TF-IDF algorithm on Wikipedia data using Apache PySpark, Streamlined Real-time Data Streaming and Analytics Pipeline, Forex data pipeline using Apache Airflow\",\n",
      "\t\"Work experience\": \"Data Mining course (Computer Science Department), Samsung Electro-Mechanics Software India Bangalore Private Limited, Telerad Tech Pvt Ltd., India, Telerad Tech Pvt Ltd, India\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78f18aad-a254-47a3-814c-fb5301596f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c96a3243-45d8-4742-9d5b-2ddb1246da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c81d18a5-7f98-4f48-a571-fdadbabfc2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Skills': 'AWS (Amazon Web Services), Keras, TensorFlow, Numpy, Pandas, Matplotlib, scikit-learn, Apache PySpark, Large language models (LLM), MLOPS, Machine learning, Deep Learning/ComputerVision, NLP, Python, R, C++, SQL, Open CV, scikit-image, Apache Airflow, Git and GitHub',\n",
       " 'Projects': \"End-to-end automated AWS ML workflow for auto insurance fraud detection, Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization, Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning, Online shoppers' purchasing intentions, Real-time machine learning prediction system for taxi ride fares, Real estate price prediction, TF-IDF algorithm on Wikipedia data using Apache PySpark, Streamlined Real-time Data Streaming and Analytics Pipeline, Forex data pipeline using Apache Airflow\",\n",
       " 'Work experience': 'Data Mining course (Computer Science Department), Samsung Electro-Mechanics Software India Bangalore Private Limited, Telerad Tech Pvt Ltd., India, Telerad Tech Pvt Ltd, India'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2302c06c-613c-41e0-afde-536ee8b7260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72bf0f9d-6adc-491f-9b4a-28cce855d571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AWS (Amazon Web Services), Keras, TensorFlow, Numpy, Pandas, Matplotlib, scikit-learn, Apache PySpark, Large language models (LLM), MLOPS, Machine learning, Deep Learning/ComputerVision, NLP, Python, R, C++, SQL, Open CV, scikit-image, Apache Airflow, Git and GitHub'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('Skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7886b63-e39e-4003-b343-1c7b355a7c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"End-to-end automated AWS ML workflow for auto insurance fraud detection, Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization, Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning, Online shoppers' purchasing intentions, Real-time machine learning prediction system for taxi ride fares, Real estate price prediction, TF-IDF algorithm on Wikipedia data using Apache PySpark, Streamlined Real-time Data Streaming and Analytics Pipeline, Forex data pipeline using Apache Airflow\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('Projects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "156b8d21-bd3b-4e6d-9686-44098cf50c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Mining course (Computer Science Department), Samsung Electro-Mechanics Software India Bangalore Private Limited, Telerad Tech Pvt Ltd., India, Telerad Tech Pvt Ltd, India'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('Work experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ff53b-fbc8-4baa-b599-b28db096c69c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec0de507-68e0-440a-baa3-7813511be479",
   "metadata": {},
   "source": [
    "## Demonstrating Sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ed23721-8911-470b-8430-9caabdbf9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2d04c81-d649-44df-9be5-75fc03c1cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3c51b37-ba25-4a36-8aa8-70b566d4ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "508c0b26-4ce0-4ef9-a491-4705cdb7010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Skills: what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\"\n",
    "    \"\\n\\n{resume_doc}\"\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"skills\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30afda7c-c817-4350-ae54-752acbf4b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you name what the job roles among Data Scientist, Machine learning Engineer, Software Engineer, Data Engineer, Devops Engineer, Cloud Architect. Are suited based on the given skill sets\"\n",
    "    \"\\n\\n{skills}\"\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"job_titles\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e57517dc-af97-40fb-8c1f-c78e0ee0f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain each skill as for what kind of projects are these usefull:\\n\\n{skills}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"skills_explanation\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29d11965-38f4-4a69-851a-d09679262f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three],\n",
    "    input_variables=[\"resume_doc\"],\n",
    "    output_variables=[\"skills\", \"job_titles\",\"skills_explanation\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74cbecff-a9e1-455a-b8a4-464c9ba15621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "seqchain_output = overall_chain(resume_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82e3371e-c379-4542-9b6e-516572a807c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(seqchain_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20168e3f-5c85-4ff7-94a9-48ecd529847f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['resume_doc', 'skills', 'job_titles', 'skills_explanation'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqchain_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8bc144d-74b9-4851-bfe9-504bf539ca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWS (Amazon Web Services)', 'Keras', 'TensorFlow', 'Numpy', 'Pandas', 'Matplotlib', 'scikit-learn', 'Apache PySpark', 'Large language models (LLM)', 'MLOPS', 'Machine learning', 'Deep Learning/ComputerVision', 'NLP', 'Python', 'R', 'C++', 'SQL', 'Open CV', 'scikit-image', 'Apache Airflow', 'Git', 'GitHub']\n"
     ]
    }
   ],
   "source": [
    "print(seqchain_output['skills'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c803e4b-4d2a-42d8-a110-9c1643114368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given skill sets, the job roles that are suited are:\n",
      "\n",
      "1. Data Scientist: Machine learning, Deep Learning/Computer Vision, NLP, Python, R, SQL, Git, GitHub, TensorFlow, Keras, Numpy, Pandas, Matplotlib, scikit-learn.\n",
      "2. Machine Learning Engineer: Machine learning, Deep Learning/Computer Vision, NLP, Python, R, SQL, Git, GitHub, TensorFlow, Keras, Numpy, Pandas, Matplotlib, scikit-learn.\n",
      "3. Software Engineer: Python, R, C++, SQL, Git, GitHub.\n",
      "4. Data Engineer: Python, SQL, Apache PySpark, Git, GitHub, Apache Airflow.\n",
      "5. DevOps Engineer: Python, Git, GitHub, Apache Airflow, AWS (Amazon Web Services).\n",
      "6. Cloud Architect: AWS (Amazon Web Services), Python, Git, GitHub.\n",
      "\n",
      "Please note that job roles can vary across different companies and industries, and these suggestions are based on the provided skill sets.\n"
     ]
    }
   ],
   "source": [
    "print(seqchain_output['job_titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b28d82b-d9b5-4023-9798-a94b4aceac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. AWS (Amazon Web Services): Useful for projects involving cloud computing and storage, scalable applications, and managing infrastructure on the cloud.\n",
      "\n",
      "2. Keras: Useful for projects involving deep learning and building neural networks. It provides a high-level interface for working with TensorFlow.\n",
      "\n",
      "3. TensorFlow: Useful for projects involving machine learning and deep learning. TensorFlow is a popular open-source framework for building and training machine learning models.\n",
      "\n",
      "4. Numpy: Useful for projects involving numerical computing and data manipulation. Numpy is a powerful library for performing mathematical operations and working with multi-dimensional arrays.\n",
      "\n",
      "5. Pandas: Useful for projects involving data analysis and manipulation. Pandas provides tools for manipulating and analyzing structured data, such as CSV files or SQL tables.\n",
      "\n",
      "6. Matplotlib: Useful for projects involving data visualization. Matplotlib is a plotting library in Python that enables the creation of various types of plots and charts.\n",
      "\n",
      "7. scikit-learn: Useful for projects involving machine learning tasks, such as classification, regression, and clustering. scikit-learn provides a wide range of algorithms and tools for machine learning.\n",
      "\n",
      "8. Apache PySpark: Useful for projects involving big data processing and distributed computing. PySpark is the Python library for Apache Spark, which provides an interface for processing large datasets in parallel.\n",
      "\n",
      "9. Large language models (LLM): Useful for projects involving natural language processing (NLP) and generating human-like text. LLMs, such as GPT-3, are capable of understanding and generating human language.\n",
      "\n",
      "10. MLOPS: Useful for projects involving machine learning operations and deployment. MLOPS refers to the practices and tools used to streamline and automate the lifecycle of machine learning models.\n",
      "\n",
      "11. Machine learning: Useful for projects involving the development and implementation of algorithms that enable systems to learn and make predictions from data.\n",
      "\n",
      "12. Deep Learning/Computer Vision: Useful for projects involving advanced image and pattern recognition tasks. Deep learning and computer vision techniques are used to analyze and understand visual data.\n",
      "\n",
      "13. NLP (Natural Language Processing): Useful for projects involving the processing and understanding of human language. NLP techniques are used for tasks such as sentiment analysis, text classification, and language translation.\n",
      "\n",
      "14. Python: Useful for a wide range of projects due to its versatility and extensive library ecosystem. Python is a popular programming language for data analysis, machine learning, and web development.\n",
      "\n",
      "15. R: Useful for projects involving statistical analysis and data visualization. R is a programming language specifically designed for statistical computing and graphical representation of data.\n",
      "\n",
      "16. C++: Useful for projects requiring high-performance computing and system-level programming. C++ is often used in projects involving embedded systems, game development, and performance-critical applications.\n",
      "\n",
      "17. SQL: Useful for projects involving the management and querying of relational databases. SQL is used to store, retrieve, and manipulate structured data.\n",
      "\n",
      "18. Open CV: Useful for projects involving computer vision and image processing. OpenCV is a library that provides tools and algorithms for analyzing and manipulating images and videos.\n",
      "\n",
      "19. scikit-image: Useful for projects involving image processing and analysis. scikit-image is a Python library that provides a collection of algorithms for image enhancement, segmentation, and feature extraction.\n",
      "\n",
      "20. Apache Airflow: Useful for projects involving data pipelines and workflow management. Apache Airflow is an open-source platform for programmatically authoring, scheduling, and monitoring workflows.\n",
      "\n",
      "21. Git: Useful for projects involving version control and collaboration. Git is a widely used distributed version control system that helps manage changes to source code and facilitates collaboration among developers.\n",
      "\n",
      "22. GitHub: Useful for projects involving code hosting, sharing, and collaboration. GitHub is a web-based platform that provides hosting for Git repositories and offers additional features such as issue tracking and pull requests.\n"
     ]
    }
   ],
   "source": [
    "print(seqchain_output['skills_explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835411f-6b55-4487-92a8-24fde4b6d0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87faaf1b-7813-4f5d-aa85-261fb8832b92",
   "metadata": {},
   "source": [
    "## Demostrating langchain tools and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a402a81-b793-4aa0-a2cd-14ee8be8e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec64e86f-cee8-4136-a9f9-657bf52195f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def job_desription(text: str)-> str:\n",
    " \"\"\"Returns job disriptions mentioned below, use this for any \\\n",
    " questions related to knowing the job disription. \\\n",
    " The input should always be an empty string, \\\n",
    " and this function will always return a string containing job disriptions.\\ \"\"\"\n",
    "    \n",
    " return \"Job discriptions:\\\n",
    " 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion.\\\n",
    " 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00e8fe51-025b-4b1c-bb65-7bdfff234821",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=turbo_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a257b223-438b-4111-891c-d1883daf5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools+ [job_desription], \n",
    "    turbo_llm,  #turbo_llm, qa_chain,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c6d74d93-bd50-413d-b1d4-d6a8aa20f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "308b971b-5197-46e7-89e7-c4c8d7db6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_template = \"\"\"\\\n",
    "The following is the resume and query:\n",
    "\n",
    "resume: {resume}\n",
    "\n",
    "query: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=agent_template)\n",
    "query_human = 'Skills: what are the technical and non technical skills? \\Answer output them as a comma separated Python list.'\n",
    "messages = prompt.format_messages(resume=resume_doc, \n",
    "                                query=query_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ccd655a-fa7c-46b7-88e5-df48239e7188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='The following is the resume and query:\\n\\nresume: NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\\n\\nSUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\\n\\nTECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\\n\\nProgramming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\\n\\nEDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\\n\\nVisvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\\n\\nJuly 2018\\n\\nWORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\\n\\nSeptember 2023 - Present\\n\\nEngineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \\uf06c\\n\\nMay 2021 – June 2022\\n\\nDeveloped a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\\n\\nAI Engineer ( Telerad Tech Pvt Ltd., India ) \\uf06c\\n\\nAugust 2018 – May 2021\\n\\nLed a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nINTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \\uf06c\\n\\nJuly 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\\n\\nACADEMIC PROJECT \\uf06c\\n\\nEnd-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers\\' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nCERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\\n\\nquery: Skills: what are the technical and non technical skills? \\\\Answer output them as a comma separated Python list.\\n')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "958138b8-1db0-4518-ba0a-35eb500f0564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to extract the technical and non-technical skills from the resume.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"job_desription\",\n",
      "  \"action_input\": \"\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mJob discriptions: 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion. 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the job descriptions for Machine Learning Engineer and Computer Vision Engineer. Now I can extract the technical and non-technical skills from these descriptions.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"job_desription\",\n",
      "  \"action_input\": \"Machine Learning Engineer\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mJob discriptions: 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion. 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the job description for Machine Learning Engineer. Now I can extract the technical and non-technical skills from this description.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"job_desription\",\n",
      "  \"action_input\": \"Computer Vision Engineer\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mJob discriptions: 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion. 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have found the job description for Computer Vision Engineer. Now I can extract the technical and non-technical skills from this description.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"job_desription\",\n",
      "  \"action_input\": \"Computer Vision Engineer\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mJob discriptions: 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion. 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have extracted the technical and non-technical skills from the job descriptions of Machine Learning Engineer and Computer Vision Engineer. Now I can provide the answer to the query.\n",
      "\n",
      "Final Answer: The technical and non-technical skills mentioned in the resume are:\n",
      "Technical Skills: AWS (Amazon Web Services), Keras, TensorFlow, Numpy, Pandas, Matplotlib, scikit-learn, Apache PySpark, Open CV, scikit-image, Apache Airflow, Git and GitHub.\n",
      "Non-Technical Skills: MLOPS, Machine learning, Deep Learning/Computer Vision, NLP.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c42c4a3f-b9b8-43e4-9983-a704d284d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_template = \"\"\"\\\n",
    "The following is the resume and query:\n",
    "\n",
    "resume: {resume}\n",
    "\n",
    "query: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=agent_template)\n",
    "query_human = 'Give me the available job discriptions?'\n",
    "messages = prompt.format_messages(resume=resume_doc, \n",
    "                                query=query_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7877247b-4195-4c72-92d9-01b6c2f6d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: Thought: The user wants to know the available job descriptions.\n",
      "Action: I will use the `job_desription` tool to get the job descriptions.\n",
      "\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to use the `job_desription` tool to get the job descriptions mentioned in the resume.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"job_desription\",\n",
      "  \"action_input\": \"\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mJob discriptions: 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion. 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe available job descriptions mentioned in the resume are:\n",
      "\n",
      "1) Machine Learning Engineer: Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion.\n",
      "\n",
      "2) Computer Vision Engineer: Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\n",
      "\n",
      "Final Answer: The available job descriptions mentioned in the resume are Machine Learning Engineer and Computer Vision Engineer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb7cca-4650-4d0c-a703-ed4705e59fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97748a8b-c721-4513-9cfb-7298ddedc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_template = \"\"\"\\\n",
    "The following is the resume and query:\n",
    "\n",
    "resume: {resume}\n",
    "\n",
    "query: {query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=agent_template)\n",
    "query_human = 'Give description about the companies mentioned in resume?'\n",
    "messages = prompt.format_messages(resume=resume_doc, \n",
    "                                query=query_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab70f683-9c72-4405-a35b-9dd5a66ddb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Samsung Electro-Mechanics Software India Bangalore Private Limited\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Bosch (company)\n",
      "Summary: Robert Bosch GmbH (; German: [bɔʃ] ), commonly known as Bosch (stylised as BOSCH), is a German multinational engineering and technology company headquartered in Gerlingen, Germany. The company was founded by Robert Bosch in Stuttgart in 1886. Bosch is 94% owned by the Robert Bosch Stiftung, a charitable institution. Although the charity is funded by owning the vast majority of shares, it has no voting rights and is involved in health and social causes unrelated to Bosch's business.\n",
      "Bosch's core operating areas are spread across four business sectors: mobility (hardware and software), consumer goods (including household appliances and power tools), industrial technology (including drive and control) and energy and building technology. In terms of revenue, Bosch is the largest automotive supplier. Moreover, it is the biggest supplier of the services in the world.\n",
      "\n",
      "Page: Apple Inc.\n",
      "Summary: Apple Inc. is an American multinational technology company headquartered in Cupertino, California. As of March 2023, Apple is the world's largest company by market capitalization, and with US$394.3 billion the largest technology company by 2022 revenue. As of June 2022, Apple is the fourth-largest personal computer vendor by unit sales; the largest manufacturing company by revenue; and the second-largest mobile phone manufacturer in the world. It is considered one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Meta (parent company of Facebook), and Microsoft.\n",
      "Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers. Apple went public in 1980 to instant financial success. The company developed computers featuring innovative graphical user interfaces, including the 1984 original Macintosh, announced that year in a critically acclaimed advertisement called \"1984\". By 1985, the high cost of its products, and power struggles between executives, caused problems. Wozniak stepped back from Apple and pursued other ventures, while Jobs resigned and founded NeXT, taking some Apple employees with him.\n",
      "As the market for personal computers expanded and evolved throughout the 1990s, Apple lost considerable market share to the lower-priced duopoly of the Microsoft Windows operating system on Intel-powered PC clones (also known as \"Wintel\"). In 1997, weeks away from bankruptcy, the company bought NeXT to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company. Over the next decade, Jobs guided Apple back to profitability through a number of tactics including introducing the iMac, iPod, iPhone and iPad to critical acclaim, launching the \"Think different\" campaign and other memorable advertising campaigns, opening the Apple Store retail chain, and acquiring numerous companies to broaden the company's product portfolio. When Jobs resigned in 2011 for health reasons, and died two months later, he was succeeded as CEO by Tim Cook.\n",
      "Apple became the first publicly traded U.S. company to be valued at over $1 trillion in August 2018, then at $2 trillion in August 2020, and at $3 trillion in January 2022. In June 2023, it was valued at just over $3 trillion. The company receives criticism regarding the labor practices of its contractors, its environmental practices, and its business ethics, including anti-competitive practices and materials sourcing. Nevertheless, the company has a large following and enjoys a high level of brand loyalty. It has also been consistently ranked as one of the world's most valuable brands.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTo give a description of the companies mentioned in the resume, I can use the Wikipedia tool to search for information about each company.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Telerad Tech Pvt Ltd., India\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382b657-173a-4589-a750-94ac20bac15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9224006-b613-4917-b0ed-8577a9321326",
   "metadata": {},
   "source": [
    "## Demonstrating Multi Prompt Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e83059f-1bf2-473d-bd7f-898999aec6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_template = \"\"\"\n",
    "You are good at matching available job description with resume.\\\n",
    "Steps:\\\n",
    "1.Retreive job discriptions from givel tool attached with agent \\\n",
    "2.Compare if resume can be selected based on any job discription, if yes then retuen that specific job discription\n",
    "3.If no job discription matches the return None\n",
    "\n",
    "Here is a resume:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "portfolio_finder_template = \"\"\"\n",
    "\n",
    "You are good at finding portfolio link from the given resume and return that link to the user.If link not found return None.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "summary_template = \"\"\"\n",
    "You are good at summerising the given resume. You will include skills, professional experience, education in the summary. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c6f4aec-48ea-41ea-bd31-2037df439fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"job_description\", \n",
    "        \"description\": \"Good for providing job discription that is matched\", \n",
    "        \"prompt_template\": job_description_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"portfolio\", \n",
    "        \"description\": \"Good for returning portfolio link from resume\", \n",
    "        \"prompt_template\": portfolio_finder_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"summary\", \n",
    "        \"description\": \"Good for providing summary of resume\", \n",
    "        \"prompt_template\": summary_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6247f7a8-7b7c-4120-afbb-7b4c4ea5367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "50ab93a3-23e9-4228-8bbf-7fd2440c0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1e5321c7-87f1-4465-ab5c-bd7f19aaabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    if name == \"job_description\":\n",
    "        chain = agent\n",
    "    elif name == \"portfolio\" :\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    else:\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        \n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1faf14a-32a9-453b-af0c-d92c5b74a6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job_description: Good for providing job discription that is matched',\n",
       " 'portfolio: Good for returning portfolio link from resume',\n",
       " 'summary: Good for providing summary of resume']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e65eef64-279b-4fec-a110-650155c00593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'job_description: Good for providing job discription that is matched\\nportfolio: Good for returning portfolio link from resume\\nsummary: Good for providing summary of resume'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "58877409-c043-4988-b3c6-1fdc59fd4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c0ae0f4-9fe0-4bd5-857a-3436cb7caf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e8c6e5fd-6acb-4f89-9309-8b44399e65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a0ea89b-d9bb-47b2-afda-9897ff4f2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c225dfdc-cc97-49c7-b16e-2e5ea9c19c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_template1 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Portfolio link: Extract portfolio link from given document.\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "review_template2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Give matching job_description, if nothing matches give None.\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db19183f-e266-4f5a-a735-db18dfd6b4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are good at summerising the given resume. You will include skills, professional experience, education in the summary. \n",
      "\n",
      "Here is a question:\n",
      "{input}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template1 = ChatPromptTemplate.from_template(review_template1)\n",
    "messages1 = prompt_template1.format_messages(text=resume_doc[:]) #resume_doc[300:] to test portfolio link :None\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b3dea16-5a8e-4012-92d4-7f5117141ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navee\\anaconda3\\envs\\llm_project\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portfolio: {'input': 'For the following text, extract the following information:\\n\\nPortfolio link: Extract portfolio link from given document.\\n\\ntext: NAVEEN RAJU S G +1 312 912 2878 | nsreeramarajugovinda@hawk.iit.edu | https://www.linkedin.com/in/naveen-raju-s-g-bb1486124 Github - https://github.com/naveenrajusg?tab=repositories | Recommendations - http://bit.ly/3QkDqD6 Portfolio - https://naveenrajusg.github.io/Portfolio/\\n\\nSUMMARY I am a seasoned professional with 4 years of experience in the field of Artificial Intelligence. Proven track record in working on Machine Learning, MLOps, and Deep Learning/Computer Vision projects. I have also excelled in project leadership, Agile methodology, and technical instruction. I am seeking a full time, Co-Op or internship in the fields of Machine Learning, MLOPS, Generative AI (LLM), Deep Learning, Computer Vision, Data Science.\\n\\nTECHNICAL SKILLS Cloud : AWS (Amazon Web Services) Deep learning framework : Keras, TensorFlow Other libraries : Numpy, Pandas, Matplotlib, scikit-learn Distributed programming : Apache PySpark Generative AI : Large language models (LLM) Other relevant skills : MLOPS, Machine learning, Deep Learning/ComputerVision, NLP\\n\\nProgramming languages : Python, R, C++, SQL Image processing libraries : Open CV, scikit-image Data Pipelining : Apache Airflow Version Control : Git and GitHub\\n\\nEDUCATION Illinois Institute of Technology Master of Science Artificial Intelligence - GPA: 3.833/4 May 2024 Courses: Machine Learning, Data Mining, Applied Statistics,Big Data Technologies, Data Preparation and Analysis, Computer Vision, Natural Language Processing, Deep Learning\\n\\nVisvesvaraya Technological University, India Bachelor of Engineering, Information Science and Engineering\\n\\nJuly 2018\\n\\nWORK EXPERIENCE Graduate Teaching Assistance - Data Mining course (Computer Science Department)\\n\\nSeptember 2023 - Present\\n\\nEngineer CL2-I ( Samsung Electro-Mechanics Software India Bangalore Private Limited ) \\uf06c\\n\\nMay 2021 – June 2022\\n\\nDeveloped a deep learning algorithm for number plate detection and recognition with 90% accuracy and improved the crowd detection, human tress pass detection algorithm by 20%.\\n\\nAI Engineer ( Telerad Tech Pvt Ltd., India ) \\uf06c\\n\\nAugust 2018 – May 2021\\n\\nLed a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision. Developed and deployed several Deep learning algorithms and image processing pipelines for medical image analysis, including lymph node segmentation (dice score of 90%, sensitivity of 90%, specificity of 90%), asymmetry detection (F1 score of 95%), and calcification reduction (recall of 92%, precision of 90%). Customized and implemented several Deep learning architectures for lung disease segmentation and detection, with dice scores of 95%, MAP scores of 90%, and overall accuracy of 93%.\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nINTERNSHIP AI-Intern ( Telerad Tech Pvt Ltd, India ) \\uf06c\\n\\nJuly 2018 Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\\n\\nACADEMIC PROJECT \\uf06c\\n\\nEnd-to-end automated AWS ML workflow for auto insurance fraud detection : Developed on AWS, integrating data processing, model training, evaluation, bias check, explainability, registration, and deployment using AWS cloud services. Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization : A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning : Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. Online shoppers\\' purchasing intentions : Project focused on analyzing data, leveraging exploratory data analysis and implementing machine learning models (classification and clustering) to comprehend purchasing behavior and predict future purchases, enabling the development of targeted marketing strategies. Real-time machine learning prediction system for taxi ride fares : Built and deployed the project using AWS SageMaker, Kinesis Data stream, Lambda functions, and S3, utilizing the New York taxi dataset and Linear Learner algorithm, with predictions stored in an S3 bucket. Real estate price prediction : Led a Comprehensive statistical analysis, applying various regression techniques to enhance prediction. This involved addressing data challenges and significantly improving the effectiveness of the regression model through thorough evaluation. TF-IDF algorithm on Wikipedia data using Apache PySpark :Developed a custom search engine utilizing Apache PySpark and AWS EMR Studio, enabling efficient analysis and ranking of relevant documents at scale. Streamlined Real-time Data Streaming and Analytics Pipeline : Implemented with AWS Kinesis, Firehose, Data Streams, AWS kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for Querying S3-backed Databases. Forex data pipeline using Apache Airflow : Pipeline included tasks for API validation, data retrieval, storage, Spark processing, and timely notifications via Email and Slack. Established task dependencies and enabled convenient DAG triggering through the Airflow UI\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\n\\uf06c\\n\\nCERTIFICATIONS AWS Certified Machine learning Speciality 2023 - Hands On | Generative AI with Large Language Models | Neural Networks and Deep Learning | Improving Deep Neural Networks: Hyper parameter tuning, Regularization and Optimization | Convolutional Neural Network | Introduction to TensorFlow for Artificial Intelligence, Machine Learning and Deep Learning | Convolutional Neural Networks in TensorFlow | Sequence Models | Apache Airflow | Apache Spark\\n'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res=chain.run(messages1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bfabe83b-b7f8-4994-8bf7-90bcba4bcd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b2b1a2d-d909-497e-8ec3-d6f73143f48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio link: https://naveenrajusg.github.io/Portfolio/\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1f2cd-cb93-49bf-b421-61f4aa995cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3acd1-28ae-4384-9a99-00f3cd7d6516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "feafeb91-2ef6-47ff-8001-34d8a236678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "abccfa1e-a3b6-47dc-bfa5-df8b0a456e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_link_schema = ResponseSchema(name=\"portfolio_link\",\n",
    "                             description=\"Give portfolio link from the given resume and return that link to the user\\\n",
    "Answer output them as a comma separated Python list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bc84397f-9696-425f-9d21-d3b7347d660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [portfolio_link_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "18ac81f2-e468-4b65-b1d3-f4fbba4f732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bcf8722f-bee3-4eaa-871b-fa1973ef660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75224ac6-d29e-4d32-8ae0-beca4723ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"portfolio_link\": string  // Give portfolio link from the given resume and return that link to the userAnswer output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "76469c5f-fd89-4e59-80fd-d2fc184b4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template1 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Portfolio link: Extract portfolio link from given document.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "98110bbf-3fca-46b5-8f47-973e20d25a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template1 = ChatPromptTemplate.from_template(review_template1)\n",
    "messages1 = prompt_template1.format_messages(text=res,format_instructions=format_instructions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1a5064c8-c152-4bdd-8f9f-abf7b981a73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "Portfolio link: Extract portfolio link from given document.\n",
      "\n",
      "text: Portfolio link: https://naveenrajusg.github.io/Portfolio/\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"portfolio_link\": string  // Give portfolio link from the given resume and return that link to the userAnswer output them as a comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages1[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "56a999bd-9207-4799-98f9-53ea7e211437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='For the following text, extract the following information:\\n\\nPortfolio link: Extract portfolio link from given document.\\n\\ntext: Portfolio link: https://naveenrajusg.github.io/Portfolio/\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"portfolio_link\": string  // Give portfolio link from the given resume and return that link to the userAnswer output them as a comma separated Python list.\\n}\\n```\\n')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a70cd2dc-8ab3-4877-b4e8-8bc335bde8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = turbo_llm_memory(messages1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a23a2c66-5870-4884-8d52-752b42904c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"portfolio_link\": \"https://naveenrajusg.github.io/Portfolio/\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d286333e-f7ed-4ffc-ae0e-bc6eb0018837",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e271af22-9510-421f-b8df-832b6682da88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'portfolio_link': 'https://naveenrajusg.github.io/Portfolio/'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e8d5f94-a290-4375-abfe-af40995f0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = output_dict.get('portfolio_link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f9583aae-a474-442c-971a-a0bdd027e612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://naveenrajusg.github.io/Portfolio/'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9b42b-3281-44e0-b621-93ff8a196519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb8a709f-63f4-44b8-9cc9-1de3c0daf367",
   "metadata": {},
   "source": [
    "## extract portfolio link from resume in previous step and then QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ef758967-251d-4924-a813-b02eef439137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AsyncHtmlLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b3a04e27-ce6e-4f9e-92c2-33f7a44f9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AsyncHtmlLoader([link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "25ad9884-9851-4fea-9c3b-1f8a8e875f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|####################################################################| 1/1 [00:00<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "html = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bfdb8688-894b-4704-aafc-1805ec57734b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='<!DOCTYPE html>\\r\\n<html lang=\"en\">\\r\\n    <head><!-- Global site tag (gtag.js) - Google Analytics -->\\r\\n        <script async src=\"https://www.googletagmanager.com/gtag/js?id=G-JZWJ68MYX4\"></script>\\r\\n        <script>\\r\\n          window.dataLayer = window.dataLayer || [];\\r\\n          function gtag(){dataLayer.push(arguments);}\\r\\n          gtag(\\'js\\', new Date());\\r\\n        \\r\\n          gtag(\\'config\\', \\'G-JZWJ68MYX4\\');\\r\\n        </script>\\r\\n        <!-- Google Tag Manager -->\\r\\n<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\r\\n    new Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\r\\n    j=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\r\\n    \\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\r\\n    })(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-P9ZS443\\');</script>\\r\\n    <!-- End Google Tag Manager -->\\r\\n        <meta charset=\"utf-8\" />\\r\\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" />\\r\\n        <meta name=\"description\" content=\"\" />\\r\\n        <meta name=\"author\" content=\"\" />\\r\\n        <title>Portfolio - Naveen Raju S G</title>\\r\\n        <link rel=\"icon\" type=\"image/x-icon\" href=\"assets/img/favicon.ico\" />\\r\\n        <!-- Font Awesome icons (free version)-->\\r\\n        <script src=\"https://use.fontawesome.com/releases/v5.13.0/js/all.js\" crossorigin=\"anonymous\"></script>\\r\\n        <!-- Google fonts-->\\r\\n        <link href=\"https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700\" rel=\"stylesheet\" type=\"text/css\" />\\r\\n        <link href=\"https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i\" rel=\"stylesheet\" type=\"text/css\" />\\r\\n        <!-- Core theme CSS (includes Bootstrap)-->\\r\\n        <link href=\"css/styles.css\" rel=\"stylesheet\" />\\r\\n        <!-- Global site tag (gtag.js) - Google Analytics -->\\r\\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-189123490-1\">\\r\\n</script>\\r\\n<script>\\r\\n  window.dataLayer = window.dataLayer || [];\\r\\n  function gtag(){dataLayer.push(arguments);}\\r\\n  gtag(\\'js\\', new Date());\\r\\n\\r\\n  gtag(\\'config\\', \\'UA-189123490-1\\');\\r\\n</script>\\r\\n    </head>\\r\\n    <body id=\"page-top\" style=\"text-size-adjust: auto;\">\\r\\n        <!-- Google Tag Manager (noscript) -->\\r\\n<noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-P9ZS443\"\\r\\n    height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\\r\\n    <!-- End Google Tag Manager (noscript) -->\\r\\n        <!-- Navigation-->\\r\\n        <nav class=\"navbar navbar-expand-lg navbar-dark bg-primary fixed-top\" id=\"sideNav\">\\r\\n            <a class=\"navbar-brand js-scroll-trigger\" href=\"#page-top\">\\r\\n                <span class=\"d-block d-lg-none\">Naveen Raju S G</span>\\r\\n                <span class=\"d-none d-lg-block\"><img class=\"img-fluid img-profile rounded-circle mx-auto mb-2\" src=\"assets/img/profile.jpg\" alt=\"\" /></span>\\r\\n            </a>\\r\\n            <button class=\"navbar-toggler\" type=\"button\" data-toggle=\"collapse\" data-target=\"#navbarSupportedContent\" aria-controls=\"navbarSupportedContent\" aria-expanded=\"false\" aria-label=\"Toggle navigation\"><span class=\"navbar-toggler-icon\"></span></button>\\r\\n            <div class=\"collapse navbar-collapse\" id=\"navbarSupportedContent\">\\r\\n                <ul class=\"navbar-nav\">\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#about\">About</a></li>\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#experience\">Work Experience</a></li>\\r\\n                    \\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#skills\">Skills</a></li>\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#education\">Education</a></li>\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#certifications\">Certifications</a></li>\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#projects\">Academic Projects</a></li>\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#PaperPublications\">Paper Publications</a></li>\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#Recommendations\">Recommendations</a></li>\\r\\n                    <li class=\"nav-item\"><a class=\"nav-link js-scroll-trigger\" href=\"#awards\">Awards</a></li>\\r\\n                    \\r\\n            \\r\\n                </ul>\\r\\n            </div>\\r\\n        </nav>\\r\\n        <!-- Page Content-->\\r\\n        <div class=\"container-fluid p-0\">\\r\\n            <!-- About-->\\r\\n            <section class=\"resume-section\" id=\"about\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h1 class=\"mb-0\">\\r\\n                        Naveen Raju S G\\r\\n                    </h1>\\r\\n                    <div class=\"subheading mb-5\">\\r\\n                        United States of America\\r\\n                    </div>\\r\\n                    <style>\\r\\n                        /* Apply left alignment to the paragraph */\\r\\n                        p {\\r\\n                          text-align: justify;\\r\\n                        }\\r\\n                      </style>\\r\\n                    <p class=\"lead mb-5\"> Hi, I\\'m Naveen Raju S G, a passionate Artificial Intelligence professional with expertise in Machine Learning, Deep Learning, and Computer Vision.  I am currently pursuing a Master of Science in Artificial Intelligence at the Illinois Institute of Technology, expecting to graduate in May 2024. I am actively seeking an internship or Co-op to leverage my expertise in Machine Learning, Deep Learning, and Computer Vision to contribute to your company\\'s projects and solve real-world business use cases. With a passion for continuous learning and a track record of developing AI solutions, I am committed to driving innovation and making a positive impact through cutting-edge AI applications.</p>\\r\\n                    <div class=\"social-icons\" style=\"text-align: right;\">\\r\\n                        <a class=\"social-icon\" href=\"https://www.linkedin.com/in/naveen-raju-s-g-bb1486124\"target=\"_blank\" > <i class=\"fab fa-linkedin-in\"></i></a>\\r\\n                        <a class=\"social-icon\" href=\"https://github.com/naveenrajusg?tab=repositories\" target=\"_blank\"><i class=\"fab fa-github\"></i></a>\\r\\n                        <a class=\"social-icon\" href=\"mailto:nsreeramarajugovinda@hawk.iit.edu\"><i class=\"fa fa-envelope\"></i></a>\\r\\n                        <a class=\"social-icon\" href=\"tel:+1 3129122878\"><i class=\"fa fa-phone\"></i></a>\\r\\n                    </div>\\r\\n                </div>\\r\\n            </section>\\r\\n            <hr class=\"m-0\" />\\r\\n            <!-- Experience-->\\r\\n            <section class=\"resume-section\" id=\"experience\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\">Work Experience</h2>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n    \\r\\n                        <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                            <div class=\"flex-grow-1\">\\r\\n                                <p class=\"subheading mb-3\" style=\"margin-top: 0px;\"><a style=\"color:#000000;\" href=\"https://drive.google.com/file/d/17tfMhYn8H25naneGA2rGSQC_s8PNls98/view?usp=sharing\" target=\"_blank\">Experience letters Hyperlink</a></p> \\r\\n                            </div>\\r\\n                        </div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <div style=\"width: 800px;\\r\\n                            padding: 1px;\\r\\n                            overflow: hidden;\" >\\r\\n                                <img src=\"assets/img/iit.jpg\" style=\"margin-right: 15px;\\r\\n                                float: left;\" alt=\"logo\" width=\"100\" height=\"100\" > \\r\\n                                <span style=\"display: block; margin:0 0 0 0;margin-left: 15px;\">\\r\\n                                <h3 style=\"margin-top: 9px; padding-top: -3px; margin-bottom: 0px;\"> Graduate Teaching Assistant </h3><p class=\"subheading mb-3\" style=\"margin-top: 0px; white-space: nowrap;\">Illinois Institute of Technology, USA</p>\\r\\n                               \\r\\n                            </span>\\r\\n                            </div>\\r\\n                            <p style=\"padding-top: 10px;\"> • Job responsibilities include supporting faculty in classroom instruction, facilitating discussion sessions, holding office hours for student consultations, evaluating assignments and exams, supervising exams, and offering constructive feedback on assignments.\\r\\n                                <br><br>• As a Graduate Teaching Assistant for the Data Mining course, I handled a class of 93 students.\\r\\n                            </p>\\r\\n                        </div>\\r\\n                        <div class=\"flex-shrink-0\"><span class=\"text-primary\">September 2023 – December 2023</span></div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <div style=\"width: 800px;\\r\\n                            padding: 1px;\\r\\n                            overflow: hidden;\" >\\r\\n                                <img src=\"assets/img/SEMB.jpg\" style=\"margin-right: 15px;\\r\\n                                float: left;\" alt=\"logo\" width=\"150\" height=\"150\" > \\r\\n                                <span style=\"display: block; margin:0 0 0 0;margin-left: 15px;\">\\r\\n                                <h3 style=\"margin-top: 9px; padding-top: -3px; margin-bottom: 0px;\">Engineer CL2-I </h3><p class=\"subheading mb-3\" style=\"margin-top: 0px; white-space: nowrap;\">Samsung Electro-Mechanics Software India Bangalore Private Limited, India</p>\\r\\n                               \\r\\n                            </span>\\r\\n                            </div>\\r\\n                            <p style=\"padding-top: 10px;\"> • Worked on a deep learning based number plate detection and character recognition algorithm with overall accuracy of 90%.\\r\\n                                <br><br>• Improved the accuracy of crowd detection and human trespass detection algorithms by 25%.\\r\\n                            </p>\\r\\n                        </div>\\r\\n                        <div class=\"flex-shrink-0\"><span class=\"text-primary\">May 2021 – June 2022</span></div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <div style=\"width: 800px;\\r\\n                            padding: 1px;\\r\\n                            overflow: hidden;\" >\\r\\n                                <img src=\"assets/img/t2.png\" style=\"margin-right: 15px;\\r\\n                                float: left;\" alt=\"logo\" width=\"220\" height=\"90\" > \\r\\n                                <span style=\"display: block; margin:0 0 0 0;margin-left: 15px;\">\\r\\n                                <h3 style=\"margin-top: 9px; padding-top: -3px; margin-bottom: 0px;\"> AI Engineer </h3><p class=\"subheading mb-3\" style=\"margin-top: 0px;\">Telerad Tech Pvt Ltd., India</p> \\r\\n                            </span>\\r\\n                            </div>\\r\\n                            <p style=\"padding-top: 10px;\"> • Led a team of 4+ using the Agile software development method, including project planning, road map creation, release planning, sprint planning, and constant client interaction; assisted in developing proof-of-concept prototypes; removed project development bottlenecks; and instructed team members on technical topics such as deep learning and computer vision.\\r\\n                                <br><br>• Customized a U-Net based architecture to segment different types of lymph nodes in mammograms with specificity and sensitivity of 90% each.\\r\\n                                <br><br>• Developed a deep learning-based architecture and image processing logic to detect asymmetry in pairs of mammograms with an F1 score of 95%.\\r\\n                                <br><br>• Built an image processing algorithm and CNN classifier to detect and reduce false positives of micro, macro, and amorphous calcification in mammograms with recall of 92% and precision of 90%.\\r\\n                                <br><br>• Developed custom CNN architectures for segmentation and detection of various lung conditions, including Cardio-thoracic ratio, Pleural effusion, consolidations, and Pneumothorax, utilizing image processing techniques for localization and quantification of the detected regions with a dice score of 95%, a MAP score of 90%, and an overall accuracy of 93%.\\r\\n                            </div>\\r\\n                        <div class=\"flex-shrink-0\"><span class=\"text-primary\">August 2018 – May 2021</span></div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <div style=\"width: 800px;\\r\\n                            padding: 1px;\\r\\n                            overflow: hidden;\" >\\r\\n                                <img src=\"assets/img/t2.png\" style=\"margin-right: 15px;\\r\\n                                float: left;\" alt=\"logo\" width=\"220\" height=\"90\" > \\r\\n                                <span style=\"display: block; margin:0 0 0 0;margin-left: 15px;\">\\r\\n                                <h3 style=\"margin-top: 9px; padding-top: -3px; margin-bottom: 0px;\">AI Intern </h3><p class=\"subheading mb-3\" style=\"margin-top: 0px;\">Telerad Tech Pvt Ltd., India</p> \\r\\n                            </span>\\r\\n                            </div>\\r\\n                            <p style=\"padding:10px\"> • Customized deep learning architecture for mammogram lesion segmentation, combined with image processing logic to improve specificity and sensitivity. Obtained an overall IOU score of 94% and an F1 score of 92.5%.\\r\\n                            </p>\\r\\n                        </div>\\r\\n                        <div class=\"flex-shrink-0\"><span class=\"text-primary\">July 2018</span></div>\\r\\n                    </div>\\r\\n\\r\\n\\r\\n                    <!-- <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n    \\r\\n                        <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                            <div class=\"flex-grow-1\">\\r\\n                                <p class=\"subheading mb-3\" style=\"margin-top: 0px;\"><a style=\"color:#000000;\" href=\"https://drive.google.com/file/d/17tfMhYn8H25naneGA2rGSQC_s8PNls98/view?usp=sharing\" target=\"_blank\">Experience letters Hyperlink</p> \\r\\n                            </div>\\r\\n                        </div>\\r\\n                    </div> -->\\r\\n                    \\r\\n            </section>\\r\\n            <hr class=\"m-0\" />\\r\\n               <!-- Skills -->\\r\\n               <section class=\"resume-section\" id=\"skills\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\">Skills</h2>\\r\\n                    <div class=\"subheading mb-3\"><b>Technical Skills</b></div>\\r\\n                    <table class=\"skills__table\" >\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t<tr>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t  <td>Programming  </td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t  <td>Python, R, C++, SQL</td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t</tr>\\r\\n                                <tr>\\r\\n                                    <td>Distributed computing programming framework&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\\r\\n                                    <td>PySpark</td>\\r\\n                                  </tr>\\r\\n                                <tr>\\r\\n                                    <td>Deep learning framework  </td>\\r\\n                                    <td>Keras, TensorFlow</td>\\r\\n                                  </tr>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t<tr>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<td>Image processing libraries  </td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<td>Open CV, scikit-image</td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t</tr>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t<tr>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<td>Machine Learning libraries  </td>\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<td>scikit learn</td>\\r\\n                                </tr>\\r\\n                                <tr>\\r\\n                                    <td>Version control  </td>\\r\\n                                    <td>Git and GitHub</td>\\r\\n                                </tr>\\r\\n                                <tr>\\r\\n                                    <td>Data engineering pipeline tool  </td>\\r\\n                                    <td>Airflow</td>\\r\\n                                </tr>\\r\\n                                <tr>\\r\\n                                    <td>Cloud Computing tool  </td>\\r\\n                                    <td>Amazon Web Services (AWS)&nbsp;&nbsp;</td>\\r\\n                                </tr>\\r\\n                                <tr>\\r\\n                                    <td>LLM Application Development Tool  </td>\\r\\n                                    <td>LangChain&nbsp;&nbsp;</td>\\r\\n                                </tr>\\r\\n                              </table> \\r\\n                              &nbsp;\\r\\n                    <p style=\"padding:0px\">• Machine learning (Supervised and Unsupervised learning)\\r\\n                        <br>• Generative AI - Large Language Models (LLM)\\r\\n                        <br>• Deep learning (Image classification, Object detection, Image segmentation)\\r\\n                        <br>• Convolution neural network (CNN), Recurrent neural network (RNN), Long short term memory (LSTM), Vision Transformers\\r\\n                        <br>• Image processing\\r\\n                        <br>• Big Data Technologies\\r\\n\\r\\n                </div>\\r\\n            </section>\\r\\n            <hr class=\"m-0\" />\\r\\n            <!-- Education-->\\r\\n            <section class=\"resume-section\" id=\"education\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\">Education</h2>\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <span class=\"mb-0\" style=\"padding-top: 0px;\">\\r\\n                                <img style=\"margin-right: 15px; \\r\\n                                float: left;padding-bottom: 8px;\" src=\"assets/img/iit.jpg\" alt=\"logo\" width=\"100\" height=\"100\" class=\"mb-0\"><h3 style=\"padding-top: 0px;\">Master of Science - Artificial Intelligence</h3></span>\\r\\n                            <div style=\"display: block; margin:0 0 0 0; margin-left: 15px;\" class=\"subheading mb-3\">Illinois Institute of Technology</div>\\r\\n                            <div style=\"display: block; margin: -20px 0 0 0; margin-left: 115px;\" class=\"subheading mb-3\">GPA : 3.9/4</div>\\r\\n\\r\\n                            <b class=\"card-subheading\">\\r\\n                            <div>\\r\\n                              <tr>\\r\\n                                  <br>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Machine Learning</span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Data Preperation and Analysis</span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Big Data</span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Applied Statistics</span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Data Mining</span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Computer Vision </span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Deep Learning </span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Natural Language Processing </span></ul>\\r\\n                            <ul><span style=\"padding-left: 75px;\">• Introduction to AI </span></ul>\\r\\n                              </tr>\\r\\n                            </div>\\r\\n                        </div> </b>\\r\\n                        <div class=\"flex-shrink-0\"><span class=\"text-primary\">August 2022 - May 2024</span></div>\\r\\n                    </div>\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <span class=\"mb-0\" style=\"padding-top: 0px;\">\\r\\n                                <img style=\"padding-top:10px;margin-right: 35px;\\r\\n                                float: left;\" src=\"assets/img/vtu.png\" alt=\"logo\" width=\"130\" height=\"130\" class=\"mb-0\"><h3 style=\"padding-top: 10px;\">Bachelor of Engineering - Information Science and Engineering</h3></span>\\r\\n                            <div style=\"display: block; margin:0 0 0 0; margin-left: 15px;\" class=\"subheading mb-3\">Visvesvaraya Technological University, India</div>\\r\\n                        </div>\\r\\n                        <div class=\"flex-shrink-0\"><span class=\"text-primary\">August 2014 - July 2018</span></div>\\r\\n                    </div>\\r\\n                </div>\\r\\n            </section>\\r\\n            <hr class=\"m-0\" />\\r\\n            \\r\\n            <!-- Certificaation-->\\r\\n            <section class=\"resume-section\" id=\"certifications\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\"> Certifications</h2>\\r\\n                    <p> • <a href=\"https://www.udemy.com/certificate/UC-ab26557a-3127-4e8f-91f6-fc6b117caf33/\" target=\"_blank\">AWS Certified Machine Learning Specialty</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/verify/VCTCG9XQDFRV\" target=\"_blank\">Generative AI with Large Language Models</p>\\r\\n                    <p> • <a href=\"https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/\" target=\"_blank\">LangChain for LLM Application Development</p>\\r\\n                    <p> • <a href=\"https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain/\" target=\"_blank\">Functions, Tools and Agents with LangChain</p>         \\r\\n                    <p> • <a href=\"https://www.udemy.com/certificate/UC-ab69da79-b431-422d-aad7-e3407a5c5537/\"target=\"_blank\">Apache Airflow</p>\\r\\n                    <p> • <a href=\"https://www.udemy.com/certificate/UC-81d63f76-30cd-4e12-a053-38de3dc902b9/\"target=\"_blank\">Apache PySpark</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/verify/385BWJXH6AYX\" target=\"_blank\"> Neural Networks and Deep Learning</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/verify/SJF2WD3QKKK5?utm_source=link&utm_campaign=copybutton_certificate\" target=\"_blank\">Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/verify/XJ9G5PXQUKGD?utm_campaign=copybutton_certificate&utm_content=cert_image&utm_medium=certificate&utm_source=link\"target=\"_blank\">Convolutional Neural Networks</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/certificate/LRPX2F6HWJA2\"target=\"_blank\">Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/verify/5B9PH4P4UTPP?utm_source=link&utm_campaign=copybutton_certificate&utm_product=course\"target=\"_blank\">Convolutional Neural Networks in TensorFlow</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/verify/H9ATZMKD2YRY\"target=\"_blank\">Sequence Models</p>\\r\\n                    <p> • <a href=\"https://www.coursera.org/account/accomplishments/verify/7TEPERWS75SJ?utm_campaign=sharing_cta&utm_content=cert_image&utm_medium=certificate&utm_product=course&utm_source=link\"target=\"_blank\">Mathematics for Machine Learning: Linear Algebra</a></p>\\r\\n                    </div>\\r\\n            </section>\\r\\n            <hr class=\"m-0\" />\\r\\n            <!-- Projects-->\\r\\n            <section class=\"resume-section\" id=\"projects\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\">Academic Projects</h2>\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\"></a>Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization</a></h3>\\r\\n                            <p> • A Comprehensive Approach with Full Fine- Tuning and PEFT, Evaluated Using ROGUE Metrics. </p>\\r\\n        \\r\\n                             </div>\\r\\n                    </div>\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\"></a>Enhance Positive Summary Generation by Fine-Tuning FLAN-T5 through Reinforcement Learning</a></h3>\\r\\n                            <p> • Used a reward model that predicts either \"not hate\" or \"hate\" for the given text and also used Proximal Policy Optimization (PPO) to fine-tune and detoxify the model. </p>\\r\\n                             </div>\\r\\n                    </div>\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Statistical-Analysis-and-Modelling-of-Real-Estate-Price-Prediction\" target=\"_blank\">Statistical Analysis and Modelling of Real-Estate Price Prediction</a></h3>\\r\\n                            <p> • This project explores the concepts of statistical data analysis and modeling various regression techniques to predict the cost of occupied homes. The objective is to analyze housing data and develop accurate regression models for home price prediction. </p>\\r\\n        \\r\\n                             </div>\\r\\n                    </div>\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Finding-the-pattern-behind-the-online-shoppers-purchasing-intention\" target=\"_blank\">Finding the pattern behind the online shoppers purchasing intention </a></h3>\\r\\n                            <p> • Objective is to analyze trends in the online shoppers purchasing intention dataset using exploratory data analysis techniques, and build machine learning models to predict whether a new customer is likely to make a purchase based on their browsing and purchasing behavior.\\r\\n                             </div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Taxi-fare-prediction-using-AWS-Kinesis-and-Machine-learning\" target=\"_blank\">Real-time Machine Learning inference system using AWS cloud services to predict Taxi ride fare </a></h3>\\r\\n                            <p> • Trained and deployed a real-time ML inference system to predict New York taxi ride fares using Amazon web services: Amazon SageMaker, Amazon Kinesis Data stream, Amazon Kinesis Data Analytics, Amazon API Gateway, AWS Lambda, Apache Flink, Cloud 9 and S3 bucket.\\r\\n                            </div>\\r\\n                    </div>\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/AWS-automated-ML-pipeline\" target=\"_blank\"> Automated end-to-end ML workflow for fraud detection in auto insurance using AWS cloud services</a></h3>\\r\\n                            <p> • Processed raw data using SageMaker Processing jobs to create training, validation, and test splits in S3.\\r\\n                                <br> • Trained an XGBoost model with SageMaker training jobs, storing the trained model artifact in S3.\\r\\n                                <br> • Evaluated model performance on the test dataset using SageMaker Processing jobs, saving the evaluation report in S3.\\r\\n                                <br> • Conducted conditional checks on model performance and initiated predefined steps in SageMaker Pipelines accordingly.\\r\\n                                <br> • Utilized SageMaker Pipelines to create and register the model, check for bias, and generate model explain ability reports in S3.\\r\\n                                <br> • Deployed the model to a SageMaker Real-Time Inference endpoint using predefined steps in SageMaker Pipelines, enabling fraud prediction for auto insurance claims.\\r\\n                            </div>\\r\\n                    </div>\\r\\n\\r\\n                    \\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\" style=\"width: 800px;\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Apache-Spark-on-AWS-EMR-Studio-to-Analyze-and-Rank-Relevant-Documents\" target=\"_blank\">Creating a Search Engine with TF-IDF Algorithm for Wikipedia Data using Apache Spark on AWS EMR Studio to Analyze and Rank Relevant Documents </a></h3>\\r\\n                            <p> • Leveraged AWS EMR Studio and PySpark to develop a Search Engine, applying TF-IDF algorithm for analysis and ranking of Wikipedia documents.\\r\\n                            </div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\" style=\"width: 800px;\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Implementing-a-Streamlined-Real-time-Data-Streaming-and-Analytics-Pipeline-with-AWS-services\" target=\"_blank\">Implementing a Streamlined Real-time Data Streaming and Analytics Pipeline with AWS Services</a></h3>\\r\\n                            <p> • Designed and executed a streamlined real-time data streaming and analytics pipeline with AWS Kinesis, Firehose, Data Streams, Kinesis Analytics Application, Glue Crawler, Glue ETL, and Athena for querying S3-backed databases.\\r\\n                        </div>\\r\\n                    </div>\\r\\n\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\" style=\"width: 800px;\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Forex-data-pipeline-Airflow\" target=\"_blank\"> Forex data pipeline using Apache Airflow</a></h3>\\r\\n                            <p> Developed a Directed Acyclic Graph (DAG) for Forex data pipeline using Airflow.\\r\\n                                <br> Directed Acyclic Graph (DAG) includes:\\r\\n                                <br>&emsp;&emsp;• Check availability of forex rates\\r\\n                                <br>&emsp;&emsp;• Check availability of file having currencies to watch\\r\\n                                <br>&emsp;&emsp;• Download forex rate with Python\\r\\n                                <br>&emsp;&emsp;• Create a Hive table to store forex rates from the HDFS\\r\\n                                <br>&emsp;&emsp;• Process forex rates with Spark\\r\\n                                <br>&emsp;&emsp;• Send a Slack notification\\r\\n                                <br>&emsp;&emsp;• Add dependencies between tasks\\r\\n                        </div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\" style=\"width: 800px;\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Image-classification-using-hierarchical-based-shifted-window-Vision-Transformers\" target=\"_blank\">Image-classification-using-hierarchical-based-shifted-window-Vision-Transformers</a></h3>\\r\\n                            <p> • To demostrate how SWIN Transformer performs efficiently compared to Vision Transformer(ViT) due to hierarchial processing with shifted windows and improved long range dependency handelling. Both models where trained on subset of Food-101 dataset.\\r\\n                            </div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\" style=\"width: 800px;\"> \\r\\n                        <div class=\"flex-grow-1\" style=\"width: 800px;\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Identification_of_Salt_Regions_in_Seismic_Images_using_DeepLearning\" target=\"_blank\">Identification of Salt Regions in Seismic Images using Deep Learning</a></h3>\\r\\n                            <p> • Using Seismic Images to segment salt deposits beneath the Earth\\'s surface using Deep learning based segmentation algorithms.\\r\\n                            </div>\\r\\n                    </div>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\" style=\"width: 800px;\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <h3 > <a style=\"color:#545454;\" href=\"https://github.com/naveenrajusg/Unpaired-Image-to-Image-Translation-using-Cycle-Consistent-Adversarial-Networks\" target=\"_blank\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></h3>\\r\\n                            <p> • Zebra to Horse and vice versa image to image translation was achieved by training Cycle-Consistent Adversarial GAN with custom designed generator and descriminator.\\r\\n                            </div>\\r\\n                    </div>\\r\\n\\r\\n\\r\\n\\r\\n                    \\r\\n                    \\r\\n            </section>\\r\\n            \\r\\n            <!-- PaperPublications-->\\r\\n            <section class=\"resume-section\" id=\"PaperPublications\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\">Paper Publications</h2>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <p> • <a style=\"color:#000000;\" href=\"https://drive.google.com/file/d/1Dxis-ThOm9XIkSG84S1F66bYcebc8oPG/view?usp=drive_link\" target=\"_blank\">Pneumothorax Detection and Classification on Chest Radiographs using Artificial Intelligence - Lattice the Machine Learning Journal Volume-2 Issue-1, ISSN 2582-8312</a>\\r\\n                                <br> • <a style=\"color:#000000;\" href=\"https://drive.google.com/file/d/1yuMZjdEUYPvkLwS1fbkSa85Pjo10-OiX/view?usp=drive_link\" target=\"_blank\">Pneumonia Detection and Classification on Chest Radiographs using Deep Learning - Lattice the Machine Learning Journal Volume-2 Issue-2, ISSN 2582-8312</a>\\r\\n\\r\\n                            </div>\\r\\n                    </div>\\r\\n            </section>\\r\\n\\r\\n\\r\\n            <!-- Recommendations-->\\r\\n            <section class=\"resume-section\" id=\"Recommendations\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\">Recommendations</h2>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <p> • <a style=\"color:#000000;\" href=\"https://www.linkedin.com/in/naveen-raju-s-g-bb1486124/details/recommendations/\" target=\"_blank\">LinkedIn Recommendations Hyperlink</a>\\r\\n                            </div>\\r\\n                    </div>\\r\\n            </section>\\r\\n\\r\\n\\r\\n            \\r\\n            <!-- Awards-->\\r\\n            <section class=\"resume-section\" id=\"awards\">\\r\\n                <div class=\"resume-section-content\">\\r\\n                    <h2 class=\"mb-5\">Awards</h2>\\r\\n\\r\\n                    <div class=\"d-flex flex-column flex-md-row justify-content-between mb-5\">\\r\\n                        <div class=\"flex-grow-1\">\\r\\n                            <p> • <b><a style=\"color:#000000;\" href=\"https://lnkd.in/g63jYj3\" target=\"_blank\">Awarded star employee of the month, Telerad Tech Pvt Ltd. Bengaluru</a></b>  \\r\\n                            <br> Received the award for the following : Published two articles in international peer-reviewed journal and to be able to present the same in the Conferences and Conclaves of both Medical and Engineering fields. Demonstration of dedication in the completion of development tasks ahead of timelines. For showing motivation, self-learning skills, and research initiatives.</p>\\r\\n                              <br>\\r\\n                            <p> • <b><a style=\"color:#000000;\" href=\"https://drive.google.com/file/d/1aOgS2oVkjjkoW-xLGMw0JYUnADLsTUmh/view?usp=drive_link\" target=\"_blank\">Awarded Outstanding Overall Performer of the Department of Information Science and Engineering, Acharya Institute of Technology, for the class of 2018</a></b> <br> I was awarded for my excellent academic performance throughout the four years of my Bachelor\\'s degree, as well as for my active involvement in various technical and cultural events, where I volunteered and coordinated several events.<br>\\r\\n\\r\\n                             </div>\\r\\n                    </div>\\r\\n                \\r\\n\\r\\n            \\r\\n\\r\\n        <!-- Bootstrap core JS-->\\r\\n        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\\r\\n        <script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js\"></script>\\r\\n        <!-- Third party plugin JS-->\\r\\n        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js\"></script>\\r\\n        <!-- Core theme JS-->\\r\\n        <script src=\"js/scripts.js\"></script>\\r\\n    </body>\\r\\n</html>\\r\\n', metadata={'source': 'https://naveenrajusg.github.io/Portfolio/', 'title': 'Portfolio - Naveen Raju S G', 'description': '', 'language': 'en'})]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b07ef847-01c5-4017-b468-127055e865c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_template = \"\"\"\\\n",
    "For the following html text, summarize the content in it and also give rating out of 5 based on if that is good profile.\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "47ea7b71-87e6-4b5d-b0b5-ce00779dd6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are good at summerising the given resume. You will include skills, professional experience, education in the summary. \n",
      "\n",
      "Here is a question:\n",
      "{input}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template1 = ChatPromptTemplate.from_template(summary_template)\n",
    "messages1 = prompt_template1.format_messages(text=html)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb754228-987e-4625-8299-5a79d865873a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c78da72c-9a29-49c4-8b2e-85dacf4e690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "splits = text_splitter.split_documents(html)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a8c7373-770d-4d42-917c-84afb86b6502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fine-Tuning the FLAN T5 LLM Model for Enhanced Dialogue Summarization.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Extract project names related to  (LLM)large language models? from projects section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "eaee4ca4-e82e-4b57-b884-cf83c6329687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The individual has a Master of Science degree in Artificial Intelligence from the Illinois Institute of Technology with a GPA of 3.9/4. They have expertise in machine learning, generative AI, deep learning, convolutional neural networks, recurrent neural networks, and image processing. They also have knowledge in data mining, computer vision, deep learning, natural language processing, and introduction to AI.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Education?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfdee62-1624-44cb-979a-af9f6c039425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e8526-e057-456c-9207-6b4f3e918a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819d8ec-1e4f-4ae0-9616-38519e00fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa1918-142d-4be1-b4b6-ff253a42da14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200790e-9519-4615-b102-9a001710e73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de82a06-6af0-4404-878d-db5a6c5649db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4e699-9c13-4af2-a162-406324d320a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bb024-df10-4791-ac40-5f4dbe3b115a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786902b2-78ae-44c5-b03c-8e0f0821befa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0ef96-b977-4582-811d-4cc024db1359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
